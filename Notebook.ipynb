{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from time import time, sleep\n",
    "from pprint import pprint\n",
    "import os\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from wurm.envs import SingleSnake\n",
    "#from wurm.utils import env_consistency, CSVLogger, ExponentialMovingAverageTracker\n",
    "#from config import BODY_CHANNEL, HEAD_CHANNEL, FOOD_CHANNEL, PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = False\n",
    "LOG_INTERVAL = 100\n",
    "MAX_GRAD_NORM = 0.5\n",
    "FPS = 10\n",
    "in_channels=3\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddCoords(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: shape(batch, channel, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size, _, x_dim, y_dim = input_tensor.size()\n",
    "\n",
    "        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n",
    "        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (y_dim - 1)\n",
    "\n",
    "        xx_channel = xx_channel * 2 - 1\n",
    "        yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "\n",
    "        ret = torch.cat([\n",
    "            input_tensor,\n",
    "            xx_channel.type_as(input_tensor),\n",
    "            yy_channel.type_as(input_tensor)], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class CoordConv2D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.addcoords = AddCoords()\n",
    "        in_size = in_channels+2\n",
    "        self.conv = nn.Conv2d(in_size, out_channels, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = self.addcoords(x)\n",
    "        ret = self.conv(ret)\n",
    "        return ret\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, residual: bool, add_coords: bool = True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.residual = residual\n",
    "        if residual:\n",
    "            assert in_channels == out_channels\n",
    "        self.conv = CoordConv2D(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        if self.residual:\n",
    "            out += identity\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def feedforward_block(input_dim: int, output_dim: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAgent_Policy(nn.Module):\n",
    "    \"\"\"Implementation of baseline agent architecture from https://arxiv.org/pdf/1806.01830.pdf\"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 num_initial_convs: int,\n",
    "                 num_residual_convs: int,\n",
    "                 num_feedforward: int,\n",
    "                 feedforward_dim: int,\n",
    "                 num_actions: int,\n",
    "                 conv_channels: int = 16,\n",
    "                 num_heads: int = 1):\n",
    "        super(ConvAgent_Policy, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_initial_convs = num_initial_convs\n",
    "        self.num_residual_convs = num_residual_convs\n",
    "        self.num_feedforward = num_feedforward\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        initial_convs = [ConvBlock(self.in_channels, self.conv_channels, residual=False), ]\n",
    "        for _ in range(self.num_initial_convs - 1):\n",
    "            initial_convs.append(ConvBlock(self.conv_channels, self.conv_channels, residual=False))\n",
    "\n",
    "        self.initial_conv_blocks = nn.Sequential(*initial_convs)\n",
    "\n",
    "        residual_convs = [ConvBlock(self.conv_channels, self.conv_channels, residual=True), ]\n",
    "        for _ in range(self.num_residual_convs - 1):\n",
    "            residual_convs.append(ConvBlock(self.conv_channels, self.conv_channels, residual=True))\n",
    "\n",
    "        self.residual_conv_blocks = nn.Sequential(*residual_convs)\n",
    "\n",
    "        feedforwards = [feedforward_block(self.conv_channels, self.feedforward_dim), ]\n",
    "        for _ in range(self.num_feedforward - 1):\n",
    "            feedforwards.append(feedforward_block(self.feedforward_dim, self.feedforward_dim))\n",
    "\n",
    "        self.feedforward = nn.Sequential(*feedforwards)\n",
    "\n",
    "        self.value_head = nn.Linear(self.feedforward_dim, num_heads)\n",
    "        self.policy_head = nn.Linear(self.feedforward_dim, self.num_actions * num_heads)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        x = self.initial_conv_blocks(x)\n",
    "        x = self.residual_conv_blocks(x)\n",
    "        x = F.adaptive_max_pool2d(x, (1, 1)).view(x.size(0), -1)\n",
    "        x = self.feedforward(x)\n",
    "        values = self.value_head(x)\n",
    "        action_probabilities = self.policy_head(x)\n",
    "        return F.softmax(action_probabilities, dim=-1), values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class A2C():\n",
    "    \"\"\"Class that encapsulates the advantage actor-critic algorithm.\n",
    "\n",
    "    Args:\n",
    "        actor_critic: Module that outputs\n",
    "        gamma: Discount value\n",
    "        value_loss_fn: Loss function between values and returns i.e. Huber, MSE\n",
    "        normalise_returns: Whether or not to normalise target returns\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 gamma: float = 0.99,\n",
    "                 value_loss_fn: Callable = F.smooth_l1_loss,\n",
    "                 normalise_returns: bool = False,\n",
    "                 use_gae: bool = False,\n",
    "                 gae_lambda: float = None,\n",
    "                 dtype: torch.dtype = torch.float):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.normalise_returns = normalise_returns\n",
    "        self.use_gae = use_gae\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.value_loss_fn = value_loss_fn\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def loss(self,\n",
    "             bootstrap_values: torch.Tensor,\n",
    "             rewards: torch.Tensor,\n",
    "             values: torch.Tensor,\n",
    "             log_probs: torch.Tensor,\n",
    "             dones: torch.Tensor,\n",
    "             return_returns: bool = False):\n",
    "        \"\"\"Calculate A2C loss.\n",
    "\n",
    "        Args:\n",
    "            bootstrap_values: Vector containing estimated value of final states each trajectory.\n",
    "                Shape (num_envs, 1)\n",
    "            rewards: Rewards for trajectories. Shape: (num_envs, num_steps)\n",
    "            values: Values for trajectory states: Shape (num_envs, num_steps)\n",
    "            log_probs: Log probabilities of actions taken during trajectory. Shape: (num_envs, num_steps)\n",
    "            dones: Done masks for trajectory states. Shape: (num_envs, num_steps)\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        if self.use_gae:\n",
    "            gae = 0\n",
    "            for t in reversed(range(rewards.size(0))):\n",
    "                if t == rewards.size(0) - 1:\n",
    "                    delta = rewards[t] + self.gamma * bootstrap_values * (~dones[t]).to(self.dtype) - values[t]\n",
    "                else:\n",
    "                    delta = rewards[t] + self.gamma * values[t+1] * (~dones[t]).to(self.dtype) - values[t]\n",
    "                gae = delta + self.gamma * self.gae_lambda * (~dones[t]).to(self.dtype) * gae\n",
    "                R = gae + values[t]\n",
    "                returns.insert(0, R)\n",
    "    \n",
    "        else:\n",
    "            R = bootstrap_values * (~dones[-1]).to(self.dtype)\n",
    "            for r, d in zip(reversed(rewards), reversed(dones.int())):\n",
    "                R = r + self.gamma * R * (~d).to(self.dtype)\n",
    "\n",
    "                returns.insert(0, R)\n",
    "\n",
    "        returns = torch.stack(returns)\n",
    "        print(returns.shape)\n",
    "        if self.normalise_returns:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + EPS)\n",
    "\n",
    "        value_loss = self.value_loss_fn(values, returns).mean()\n",
    "        advantages = returns - values\n",
    "        policy_loss = - (advantages.detach() * log_probs).mean()\n",
    "\n",
    "        ret = (value_loss, policy_loss)\n",
    "        if return_returns:\n",
    "            ret += returns\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def print_vars(self):\n",
    "        print(\"Gamma:\", self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory buffer for Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryStore(object):\n",
    "    \"\"\"Stores list of transitions.\n",
    "\n",
    "    Each property should return a tensor of shape (num_steps, num_envs, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def append(self,\n",
    "               state: torch.Tensor = None,\n",
    "               action: torch.Tensor = None,\n",
    "               log_prob: torch.Tensor = None,\n",
    "               reward: torch.Tensor = None,\n",
    "               value: torch.Tensor = None,\n",
    "               done: torch.Tensor = None,\n",
    "               entropy: torch.Tensor = None,\n",
    "               hidden_state: torch.Tensor = None):\n",
    "        \"\"\"Adds a transition to the store.\n",
    "\n",
    "        Each argument should be a vector of shape (num_envs, 1)\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            self._states.append(state)\n",
    "\n",
    "        if action is not None:\n",
    "            self._actions.append(action)\n",
    "\n",
    "        if log_prob is not None:\n",
    "            self._log_probs.append(log_prob)\n",
    "\n",
    "        if reward is not None:\n",
    "            self._rewards.append(reward)\n",
    "\n",
    "        if value is not None:\n",
    "            self._values.append(value)\n",
    "\n",
    "        if done is not None:\n",
    "            self._dones.append(done)\n",
    "\n",
    "        if entropy is not None:\n",
    "            self._entropies.append(entropy)\n",
    "\n",
    "        if hidden_state is not None:\n",
    "            self._hiddens.append(hidden_state)\n",
    "\n",
    "    def clear(self):\n",
    "        self._states = []\n",
    "        self._actions = []\n",
    "        self._log_probs = []\n",
    "        self._rewards = []\n",
    "        self._values = []\n",
    "        self._dones = []\n",
    "        self._entropies = []\n",
    "        self._hiddens = []\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        return torch.stack(self._states)\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return torch.stack(self._actions)\n",
    "\n",
    "    @property\n",
    "    def log_probs(self):\n",
    "        return torch.stack(self._log_probs)\n",
    "\n",
    "    @property\n",
    "    def rewards(self):\n",
    "        return torch.stack(self._rewards)\n",
    "\n",
    "    @property\n",
    "    def values(self):\n",
    "        return torch.stack(self._values)\n",
    "\n",
    "    @property\n",
    "    def dones(self):\n",
    "        return torch.stack(self._dones)\n",
    "\n",
    "    @property\n",
    "    def entropies(self):\n",
    "        return torch.stack(self._entropies)\n",
    "\n",
    "    @property\n",
    "    def hidden_state(self):\n",
    "        return torch.stack(self._hiddens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAgent_Policy(nn.Module):\n",
    "    \"\"\"Implementation of baseline agent architecture from https://arxiv.org/pdf/1806.01830.pdf\"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 num_initial_convs: int,\n",
    "                 num_residual_convs: int,\n",
    "                 num_feedforward: int,\n",
    "                 feedforward_dim: int,\n",
    "                 num_actions: int,\n",
    "                 conv_channels: int = 16,\n",
    "                 num_heads: int = 1):\n",
    "        super(ConvAgent_Policy, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_initial_convs = num_initial_convs\n",
    "        self.num_residual_convs = num_residual_convs\n",
    "        self.num_feedforward = num_feedforward\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.conv_channels = conv_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        initial_convs = [ConvBlock(self.in_channels, self.conv_channels, residual=False), ]\n",
    "        for _ in range(self.num_initial_convs - 1):\n",
    "            initial_convs.append(ConvBlock(self.conv_channels, self.conv_channels, residual=False))\n",
    "\n",
    "        self.initial_conv_blocks = nn.Sequential(*initial_convs)\n",
    "\n",
    "        residual_convs = [ConvBlock(self.conv_channels, self.conv_channels, residual=True), ]\n",
    "        for _ in range(self.num_residual_convs - 1):\n",
    "            residual_convs.append(ConvBlock(self.conv_channels, self.conv_channels, residual=True))\n",
    "\n",
    "        self.residual_conv_blocks = nn.Sequential(*residual_convs)\n",
    "\n",
    "        feedforwards = [feedforward_block(self.conv_channels, self.feedforward_dim), ]\n",
    "        for _ in range(self.num_feedforward - 1):\n",
    "            feedforwards.append(feedforward_block(self.feedforward_dim, self.feedforward_dim))\n",
    "\n",
    "        self.feedforward = nn.Sequential(*feedforwards)\n",
    "\n",
    "        self.value_head = nn.Linear(self.feedforward_dim, num_heads)\n",
    "        self.policy_head = nn.Linear(self.feedforward_dim, self.num_actions * num_heads)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        x = self.initial_conv_blocks(x)\n",
    "        x = self.residual_conv_blocks(x)\n",
    "        x = F.adaptive_max_pool2d(x, (1, 1)).view(x.size(0), -1)\n",
    "        x = self.feedforward(x)\n",
    "        values = self.value_head(x)\n",
    "        action_probabilities = self.policy_head(x)\n",
    "        return F.softmax(action_probabilities, dim=-1), values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Environment 3-channel state representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_DEVICE = 'cpu' #change to 'cpu' if gpu not available.\n",
    "env = SingleSnake(num_envs=10, size=10, observation_mode='raw', device= DEFAULT_DEVICE)\n",
    "#env.render()\n",
    "model = ConvAgent_Policy(num_actions=4, num_initial_convs=2, in_channels=in_channels, conv_channels=32,\n",
    "                             num_residual_convs=2, num_feedforward=1, feedforward_dim=64).to(DEFAULT_DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "trajectories = TrajectoryStore()\n",
    "#ewm_tracker = ExponentialMovingAverageTracker(alpha=0.025)\n",
    "a2c = A2C(gamma = 0.99)\n",
    "\n",
    "#torch.no_grad()\n",
    "#model.eval()\n",
    "print(in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAgent_Policy(\n",
      "  (initial_conv_blocks): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): CoordConv2D(\n",
      "        (addcoords): AddCoords()\n",
      "        (conv): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (conv): CoordConv2D(\n",
      "        (addcoords): AddCoords()\n",
      "        (conv): Conv2d(34, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (residual_conv_blocks): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): CoordConv2D(\n",
      "        (addcoords): AddCoords()\n",
      "        (conv): Conv2d(34, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (conv): CoordConv2D(\n",
      "        (addcoords): AddCoords()\n",
      "        (conv): Conv2d(34, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feedforward): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (value_head): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (policy_head): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Actor-Critic 3-channel state Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "render = False\n",
    "args_entropy = 0.01\n",
    "update_steps = 50\n",
    "total_steps = 1000\n",
    "############################\n",
    "# Run agent in environment #\n",
    "############################\n",
    "t0 = time()\n",
    "state = env.reset()\n",
    "\n",
    "for i_step in count(1):\n",
    "\n",
    "    #print(\"step:\", i_step)\n",
    "    #clear_output(wait=True)\n",
    "    if render:\n",
    "        env.render()\n",
    "        sleep(1. / FPS)\n",
    "\n",
    "    #############################\n",
    "    # Interact with environment #\n",
    "    #############################\n",
    "    \n",
    "    probs, state_value = model(state)\n",
    "    \n",
    "    action_distribution = Categorical(probs)\n",
    "    \n",
    "    entropy = action_distribution.entropy().mean()\n",
    "    action = action_distribution.sample().clone().long()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    trajectories.append(\n",
    "        action=action,\n",
    "        log_prob=action_distribution.log_prob(action),\n",
    "        value=state_value,\n",
    "        reward=reward,\n",
    "        done=done,\n",
    "        entropy=entropy\n",
    "    )\n",
    "    #print(state)\n",
    "\n",
    "    \n",
    "    env.reset(done)\n",
    "\n",
    "    ##########################\n",
    "    # Advantage actor-critic #\n",
    "    ##########################\n",
    "    if i_step % update_steps == 0:\n",
    "        with torch.no_grad():\n",
    "            _, bootstrap_values = model(state)\n",
    "\n",
    "        value_loss, policy_loss = a2c.loss(bootstrap_values, trajectories.rewards, trajectories.values,\n",
    "                                           trajectories.log_probs, trajectories.dones)\n",
    "\n",
    "        entropy_loss = - trajectories.entropies.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = value_loss + policy_loss + args_entropy * entropy_loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        trajectories.clear()\n",
    "\n",
    "    ###########\n",
    "    if i_step == 1000:\n",
    "        break\n",
    "if render:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a87813ffe34d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEFAULT_DEVICE = 'cpu'\n",
    "left = torch.Tensor([1]).long().to(DEFAULT_DEVICE)\n",
    "up = torch.Tensor([2]).long().to(DEFAULT_DEVICE)\n",
    "right = torch.Tensor([3]).long().to(DEFAULT_DEVICE)\n",
    "down = torch.Tensor([0]).long().to(DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wurm.envs import SimpleGridworld\n",
    "env_test = SimpleGridworld(num_envs=10, size=10, observation_mode='one_channel', device=DEFAULT_DEVICE, auto_reset=False)\n",
    "state= env_test.reset()\n",
    "env_test.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]) tensor([True, True, True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env_test.step(env_test.random_action())\n",
    "env_test.render()\n",
    "print(reward,done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test.reset()\n",
    "env_test.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2]]).float().requires_grad_(False)\n",
    "b = torch.tensor([[1,2]]).long().requires_grad_(False)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sub_(2,b)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
