{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os #to get current working directory\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle #for storing data\n",
    "from wurm.envs import SingleSnake\n",
    "from wurm.envs import SimpleGridworld\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "DEFAULT_DEVICE = 'cuda' #set device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size: int):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer = collections.deque(maxlen=max_buffer_size)\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "    def clear_buffer(self):\n",
    "        self.buffer.clear()\n",
    "        \n",
    "    #Sample superbatches and sub sample parallel environments\n",
    "    def sample_subbatch(self,superbatch_length, subbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        sub_length = self.buffer[0][0].shape[0]\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), superbatch_length):\n",
    "            rand_int_1 = np.random.randint(0, sub_length, subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0][rand_int_1])\n",
    "            next_states.append(transition[1][rand_int_1])\n",
    "            actions.append(transition[2][rand_int_1])\n",
    "            rewards.append(transition[3][rand_int_1])\n",
    "            terminals.append(transition[4][rand_int_1])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "\n",
    "    def sample_superbatch(self,superbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), superbatch_length):\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "    \n",
    "    #sample parallel environments of subbatch_length from a randomly selected buffer location.\n",
    "    def sample(self, subbatch_length):\n",
    "            rand_int = np.random.randint(0, len(self.buffer))\n",
    "            rand_int_1 = np.random.randint(0, len(self.buffer[0][0]), subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states=transition[0][rand_int_1]\n",
    "            next_states=transition[1][rand_int_1]\n",
    "            actions=transition[2][rand_int_1]\n",
    "            rewards=transition[3][rand_int_1]\n",
    "            terminals=transition[4][rand_int_1]\n",
    "            return (states,next_states,actions,rewards,terminals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C():\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800, \n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01,\n",
    "                 lam = 10):\n",
    "\n",
    "        self.model = NN(*NN_args)\n",
    "        self.model_optim = torch.optim.Adam(self.model.parameters(), lr=lr) #set learning rate\n",
    "        self.gamma = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "    def load(self, path, device=DEFAULT_DEVICE):\n",
    "        self.model = torch.load(path, map_location=device)\n",
    "        self.model_optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "    def action(self, state):\n",
    "        #self.values and self.log_probs will be needed to update model in self.update()\n",
    "        action_probs, self.values = self.model(state)\n",
    "        dist = torch.distributions.Categorical(probs = action_probs) #defining a distribution to sample from\n",
    "        actions = dist.sample() #sampling an action from the distribution\n",
    "        self.log_probs = dist.log_prob(actions)\n",
    "        #self.entropy = dist.entropy()\n",
    "        return actions\n",
    "\n",
    "    def update(self, next_states, rewards, terminals):\n",
    "        _, next_state_values = self.model(next_states)\n",
    "        \n",
    "        returns = (rewards + (~terminals)*self.gamma*next_state_values).detach()\n",
    "        values = self.values\n",
    "        \n",
    "        value_loss = torch.nn.functional.smooth_l1_loss(returns,values).mean()\n",
    "        advantages = returns-values\n",
    "        policy_loss = -(advantages.detach()*self.log_probs).mean()\n",
    "        #entropy_loss = 0.01*self.entropy.mean()\n",
    "        loss = policy_loss + value_loss \n",
    "        \n",
    "        self.model_optim.zero_grad()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "        loss.backward()\n",
    "        self.model_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeNet(torch.nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self.layer_length = size*size\n",
    "        self.common_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "        self.policy_layer = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 2, kernel_size=(1,1), stride=(1,1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(2*self.layer_length,4),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "        self.value_layer = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 1, kernel_size=(1,1), stride=(1,1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.layer_length,64),\n",
    "            torch.nn.Linear(64,1)\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.common_layer(x)\n",
    "        self.policy = self.policy_layer(x)\n",
    "        self.value = self.value_layer(x)\n",
    "        return (self.policy, self.value.squeeze(-1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnakeNet(\n",
      "  (common_layer): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (policy_layer): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Flatten()\n",
      "    (3): Linear(in_features=200, out_features=4, bias=True)\n",
      "    (4): Softmax(dim=-1)\n",
      "  )\n",
      "  (value_layer): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Flatten()\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "environment = 'SingleSnake'\n",
    "num_envs = 1000 #Number of parallel environments to simulate. Use small value for cpu (eg. 1)\n",
    "test_num_envs = 100 #Number of parallel environments when validating\n",
    "\n",
    "if environment == 'SimpleGridworld':\n",
    "    env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=True)\n",
    "    test_env = SimpleGridworld(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=Reinforce_Agent(NN = FNN_1, NN_args = (state_dim, 512, action_dim),\n",
    "                           lr=0.005, discount = 1.0)\n",
    "\n",
    "elif environment == 'SingleSnake':\n",
    "    env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset= True)\n",
    "    test_env = SingleSnake(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    agent=A2C(NN = SnakeNet, NN_args = (10,), lr = 0.001, discount = 0.8)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid option\")\n",
    "\n",
    "#agent.load(F\"models/perfect_model.h5\") #Load an external model\n",
    "agent.train()\n",
    "print(agent.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3600\n",
      "Episode Completed: 100 / 100\n",
      "Mean, Median, Max, Min, std: 15.69 15.0 28.0 6.0 4.5270185\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "render=False\n",
    "save_model = False\n",
    "number_of_steps = 100000\n",
    "epsilon = 1.0\n",
    "####Code to compute total reward####\n",
    "\n",
    "total_reward = torch.zeros(num_envs).to(DEFAULT_DEVICE)\n",
    "step_list=[]\n",
    "fc_list=[] #food collected\n",
    "best_fc = 0\n",
    "####Code to compute total reward####\n",
    "\n",
    "\n",
    "agent.train()\n",
    "\n",
    "state=env.reset()\n",
    "#Learning\n",
    "for i in range(0,number_of_steps):\n",
    "    ##############Learning#############################\n",
    "    action = agent.action(state) \n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.update(next_state, reward, terminal)  \n",
    "    state = next_state\n",
    "\n",
    "    #############Validation############################\n",
    "    if i%100 == 0:\n",
    "        agent.evaluate()                        \n",
    "        t_state = test_env.reset()\n",
    "        fc_sum = torch.zeros((test_num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "        #hit_terminal = torch.zeros((test_num_envs,)).bool().to(DEFAULT_DEVICE)\n",
    "        for _ in range(1000): #max steps\n",
    "            t_action = agent.action(t_state)\n",
    "            t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "            #anything with a positive reward is considered as food.\n",
    "            fc_sum+=(t_reward>0).float()\n",
    "            #hit_terminal |= t_terminal\n",
    "            t_state = t_next_state\n",
    "            if t_terminal.all():\n",
    "                break\n",
    "\n",
    "        t_sum = fc_sum.cpu().numpy()\n",
    "        t_mean = np.mean(t_sum)\n",
    "        print('Step:', i)\n",
    "        print(\"Episode Completed:\", t_terminal.sum().cpu().numpy(), \"/\", test_num_envs)\n",
    "        print(\"Mean, Median, Max, Min, std:\", \n",
    "              t_mean, \n",
    "              np.median(t_sum),\n",
    "              np.max(t_sum),\n",
    "              np.min(t_sum),\n",
    "              np.std(t_sum))\n",
    "        fc_list.append(t_mean)\n",
    "        step_list.append(i)\n",
    "        plt.plot(step_list, fc_list)\n",
    "        plt.show()\n",
    "        agent.train()\n",
    "        clear_output(wait=True)\n",
    "        if t_mean>best_fc:\n",
    "            best_fc = t_mean\n",
    "            #torch.save(agent.model,\"models/best_model.h5\")\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"models/perfect_model.h5\")\n",
    "print(agent.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Record Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "agent.evaluate()\n",
    "PATH = os.getcwd()\n",
    "state = env.reset()\n",
    "for episode in range(10):\n",
    "    fc_sum = 0\n",
    "    recorder = VideoRecorder(env, path=PATH + f'/videos/PG_{episode}.mp4')\n",
    "    env.render()\n",
    "    recorder.capture_frame()\n",
    "    time.sleep(0.2)\n",
    "    counter = 0\n",
    "    while(1):\n",
    "        counter+=1\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        fc_sum+= (reward>0).cpu().numpy()\n",
    "        env.render()\n",
    "        recorder.capture_frame()\n",
    "        #time.sleep(0.2)\n",
    "        state = next_state\n",
    "        if terminal.all() or counter==1000:\n",
    "            recorder.close()\n",
    "            break\n",
    "    print(\"Completed:\", terminal.any().cpu().numpy())\n",
    "    print('Episode:', episode, 'Food Collected:', fc_sum)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Average Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "agent.evaluate()\n",
    "\n",
    "                       \n",
    "t_state = test_env.reset()\n",
    "fc_sum = torch.zeros((num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "\n",
    "for steps in range(1000): #max steps\n",
    "    t_action = agent.action(t_state)\n",
    "    t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "    #anything with a positive reward is considered as food.\n",
    "    fc_sum+=(t_reward>0).float()\n",
    "    t_state = t_next_state\n",
    "    if t_terminal.all():\n",
    "        break\n",
    "\n",
    "t_sum = fc_sum.cpu().numpy()\n",
    "\n",
    "t_mean = np.mean(t_sum)\n",
    "print(\"Completed:\", t_terminal.sum().cpu().numpy(), \"/\", num_envs)\n",
    "print(\"Mean, Median, Max, Min, std:\", \n",
    "      t_mean, \n",
    "      np.median(t_sum),\n",
    "      np.max(t_sum),\n",
    "      np.min(t_sum),\n",
    "      np.std(t_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
