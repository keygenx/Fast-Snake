{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os #to get current working directory\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle #for storing data\n",
    "from wurm.envs import SingleSnake\n",
    "from wurm.envs import SimpleGridworld\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "DEFAULT_DEVICE = 'cuda' #set device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Trajectory():\n",
    "    def __init__(self):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer_states = []\n",
    "        self.buffer_next_states = []\n",
    "        self.buffer_actions = []\n",
    "        self.buffer_rewards = []\n",
    "        self.buffer_terminals = []\n",
    "        self.buffer_log_probs = []\n",
    "        \n",
    "    def append_to_trajectory(self,\n",
    "               state: torch.Tensor = None,\n",
    "               next_state: torch.Tensor = None,\n",
    "               action: torch.Tensor = None,\n",
    "               log_prob: torch.Tensor = None,\n",
    "               reward: torch.Tensor = None,\n",
    "               value: torch.Tensor = None,\n",
    "               terminal: torch.Tensor = None,\n",
    "               entropy: torch.Tensor = None,\n",
    "               hidden_state: torch.Tensor = None):\n",
    "        \"\"\"Adds a transition to the store.\n",
    "\n",
    "        Each argument should be a vector of shape (num_envs, 1)\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            self.buffer_states.append(state)\n",
    "        if next_state is not None:\n",
    "            self.buffer_next_states.append(next_state)\n",
    "        if action is not None:\n",
    "            self.buffer_actions.append(action)\n",
    "        if log_prob is not None:\n",
    "            self.buffer_log_probs.append(log_prob)\n",
    "        if reward is not None:\n",
    "            self.buffer_rewards.append(reward)\n",
    "        if value is not None:\n",
    "            self.buffer_values.append(value)\n",
    "        if terminal is not None:\n",
    "            self.buffer_terminals.append(terminal)\n",
    "        if entropy is not None:\n",
    "            self.buffer_entropies.append(entropy)\n",
    "        if hidden_state is not None:\n",
    "            self.buffer_hiddens.append(hidden_state)\n",
    "        \n",
    "    def clear_trajectory(self):\n",
    "        self.buffer_states = []\n",
    "        self.buffer_next_states = []\n",
    "        self.buffer_actions = []\n",
    "        self.buffer_rewards = []\n",
    "        self.buffer_terminals = []\n",
    "        self.buffer_log_probs = []\n",
    "     \n",
    "    \"\"\"@property\n",
    "    def states(self):\n",
    "        return torch.stack(self.buffer_states)\n",
    "    @property\n",
    "    def next_states(self):\n",
    "        return torch.stack(self.buffer_next_states)\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return torch.stack(self.buffer_actions)\n",
    "    @property\n",
    "    def log_probs(self):\n",
    "        return torch.stack(self.buffer_log_probs)\n",
    "    @property\n",
    "    def rewards(self):\n",
    "        return torch.stack(self.buffer_rewards)\n",
    "    @property\n",
    "    def values(self):\n",
    "        return torch.stack(self.buffer_values)\n",
    "    @property\n",
    "    def terminals(self):\n",
    "        return torch.stack(self.buffer_terminals)\n",
    "    @property\n",
    "    def entropies(self):\n",
    "        return torch.stack(self.buffer_entropies)\n",
    "    @property\n",
    "    def hidden_state(self):\n",
    "        return torch.stack(self.buffer_hiddens)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def update(self):\n",
    "        G = torch.zeros(len(self.buffer_states[0])).to(DEFAULT_DEVICE)\n",
    "        for t in reversed(range(len(self.buffer_states))):\n",
    "            G.mul_(self.discount_factor)\n",
    "            G.add_(self.buffer_rewards[t])\n",
    "            \n",
    "        log_probs = self.model(self.buffer_states[0])\n",
    "        action = self.buffer_actions[0]\n",
    "        performance = log_probs*G[:,None]\n",
    "        loss = torch.nn.NLLLoss(reduction = 'sum')(performance, action)\n",
    "        self.model_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.model_optim.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(Trajectory):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800, \n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01,\n",
    "                 lam = 10):\n",
    "        super().__init__()\n",
    "        self.model = NN(*NN_args)\n",
    "        self.model_optim = torch.optim.Adam(self.model.parameters(), lr=lr) #set learning rate\n",
    "        self.gamma = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model = torch.load(path)\n",
    "        self.model_optim = torch.optim.Adam(self.qnet.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "    def action(self, state):\n",
    "        #self.values and self.log_probs will be needed to update model in self.update()\n",
    "        action_probs, self.values = self.model(state)\n",
    "        dist = torch.distributions.Categorical(probs = action_probs)\n",
    "        actions = dist.sample()\n",
    "        self.log_probs = dist.log_prob(actions)\n",
    "        return actions\n",
    "\n",
    "    def update(self, next_states, rewards, terminals):\n",
    "        _, next_state_values = self.model(next_states)\n",
    "        \n",
    "        returns = (rewards + (~terminals)*self.gamma*next_state_values).detach() \n",
    "        values = self.values\n",
    "        \n",
    "        value_loss = torch.nn.functional.smooth_l1_loss(returns,values).mean()\n",
    "        advantages = returns-values\n",
    "        policy_loss = -(advantages.detach()*self.log_probs).mean()\n",
    "        loss = policy_loss + value_loss\n",
    "        \n",
    "        self.model_optim.zero_grad()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "        loss.backward()\n",
    "        self.model_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeNet(torch.nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self.layer_length = size*size\n",
    "        self.common_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "        self.policy_layer = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 2, kernel_size=(1,1), stride=(1,1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(2*self.layer_length,4),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "        self.value_layer = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 1, kernel_size=(1,1), stride=(1,1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.layer_length,64),\n",
    "            torch.nn.Linear(64,1)\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.common_layer(x)\n",
    "        self.policy = self.policy_layer(x)\n",
    "        self.value = self.value_layer(x)\n",
    "        return (self.policy, self.value.squeeze(-1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'SingleSnake'\n",
    "num_envs = 1000 #Number of parallel environments to simulate. Use small value for cpu (eg. 1)\n",
    "test_num_envs = 100\n",
    "\n",
    "if environment == 'SimpleGridworld':\n",
    "    env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=True)\n",
    "    test_env = SimpleGridworld(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=Reinforce_Agent(NN = FNN_1, NN_args = (state_dim, 512, action_dim),\n",
    "                           lr=0.005, discount = 1.0)\n",
    "\n",
    "elif environment == 'SingleSnake':\n",
    "    env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset= True)\n",
    "    test_env = SingleSnake(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=A2C(NN = SnakeNet, NN_args = (10,), lr = 0.0005, discount =0.99)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid option\")\n",
    "\n",
    "#agent.load(\"models/best_model.h5\")\n",
    "agent.train()\n",
    "print(agent.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render=False\n",
    "save_model = False\n",
    "number_of_steps = 100000\n",
    "epsilon = 1.0\n",
    "####Code to compute total reward####\n",
    "\n",
    "total_reward = torch.zeros(num_envs).to(DEFAULT_DEVICE)\n",
    "step_list=[]\n",
    "fc_list=[] #food collected\n",
    "best_fc = 0\n",
    "####Code to compute total reward####\n",
    "\n",
    "\n",
    "agent.train()\n",
    "\n",
    "state=env.reset()\n",
    "#Learning\n",
    "for i in range(0,number_of_episodes):\n",
    "    ##############Learning######################\n",
    "\n",
    "    action = agent.action(state) \n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.update(next_state, reward, terminal)  \n",
    "    state = next_state\n",
    "\n",
    "    #############Validation############################\n",
    "    if i%100 == 0:\n",
    "        agent.evaluate()                        \n",
    "        t_state = test_env.reset()\n",
    "        fc_sum = torch.zeros((test_num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "        #hit_terminal = torch.zeros((test_num_envs,)).bool().to(DEFAULT_DEVICE)\n",
    "        for _ in range(1000): #max steps\n",
    "            t_action = agent.action(t_state)\n",
    "            t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "            #anything with a positive reward is considered as food.\n",
    "            fc_sum+=(t_reward>0).float()\n",
    "            #hit_terminal |= t_terminal\n",
    "            t_state = t_next_state\n",
    "            if t_terminal.all():\n",
    "                break\n",
    "\n",
    "        t_sum = fc_sum.cpu().numpy()\n",
    "        t_mean = np.mean(t_sum)\n",
    "        print('Step:', i)\n",
    "        print(\"Episode Completed:\", t_terminal.sum().cpu().numpy(), \"/\", test_num_envs)\n",
    "        print(\"Mean, Median, Max, Min, std:\", \n",
    "              t_mean, \n",
    "              np.median(t_sum),\n",
    "              np.max(t_sum),\n",
    "              np.min(t_sum),\n",
    "              np.std(t_sum))\n",
    "        fc_list.append(t_mean)\n",
    "        step_list.append(i)\n",
    "        plt.plot(step_list, fc_list)\n",
    "        plt.show()\n",
    "        agent.train()\n",
    "        clear_output(wait=True)\n",
    "        if t_mean>best_fc:\n",
    "            best_fc = t_mean\n",
    "            #torch.save(agent.model,\"models/best_model.h5\")\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Record Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "agent.evaluate()\n",
    "PATH = os.getcwd()\n",
    "state = env.reset()\n",
    "for episode in range(100):\n",
    "    fc_sum = 0\n",
    "    recorder = VideoRecorder(env, path=PATH + f'/videos/snake_{episode}.mp4')\n",
    "    #env.render()\n",
    "    recorder.capture_frame()\n",
    "    time.sleep(0.2)\n",
    "    counter = 0\n",
    "    while(1):\n",
    "        counter+=1\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        fc_sum+= (reward>0).cpu().numpy()\n",
    "        #env.render()\n",
    "        recorder.capture_frame()\n",
    "        #time.sleep(0.2)\n",
    "        state = next_state\n",
    "        if terminal.all() or counter==1000:\n",
    "            recorder.close()\n",
    "            break\n",
    "    print(\"Completed:\", terminal.any().cpu().numpy())\n",
    "    print('Episode:', episode, 'Food Collected:', fc_sum)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Average Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "agent.evaluate()\n",
    "\n",
    "                       \n",
    "t_state = test_env.reset()\n",
    "fc_sum = torch.zeros((num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "\n",
    "for steps in range(1000): #max steps\n",
    "    t_action = agent.action(t_state)\n",
    "    t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "    #anything with a positive reward is considered as food.\n",
    "    fc_sum+=(t_reward>0).float()\n",
    "    t_state = t_next_state\n",
    "    if t_terminal.all():\n",
    "        break\n",
    "\n",
    "t_sum = fc_sum.cpu().numpy()\n",
    "t_mean = np.mean(t_sum)\n",
    "print(\"Completed:\", t_terminal.sum().cpu().numpy())\n",
    "print(\"Mean, Median, Max, Min, std:\", \n",
    "      t_mean, \n",
    "      np.median(t_sum),\n",
    "      np.max(t_sum),\n",
    "      np.min(t_sum),\n",
    "      np.std(t_sum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
