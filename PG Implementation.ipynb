{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os #to get current working directory\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle #for storing data\n",
    "from wurm.envs import SingleSnake\n",
    "from wurm.envs import SimpleGridworld\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "DEFAULT_DEVICE = 'cuda' #set device\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Trajectory():\n",
    "    def __init__(self):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer_states = []\n",
    "        self.buffer_next_states = []\n",
    "        self.buffer_actions = []\n",
    "        self.buffer_rewards = []\n",
    "        self.buffer_terminals = []\n",
    "        self.buffer_log_probs = []\n",
    "        \n",
    "    def append_to_trajectory(self,\n",
    "               state: torch.Tensor = None,\n",
    "               next_state: torch.Tensor = None,\n",
    "               action: torch.Tensor = None,\n",
    "               log_prob: torch.Tensor = None,\n",
    "               reward: torch.Tensor = None,\n",
    "               value: torch.Tensor = None,\n",
    "               terminal: torch.Tensor = None,\n",
    "               entropy: torch.Tensor = None,\n",
    "               hidden_state: torch.Tensor = None):\n",
    "        \"\"\"Adds a transition to the store.\n",
    "\n",
    "        Each argument should be a vector of shape (num_envs, 1)\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            self.buffer_states.append(state)\n",
    "        if next_state is not None:\n",
    "            self.buffer_next_states.append(next_state)\n",
    "        if action is not None:\n",
    "            self.buffer_actions.append(action)\n",
    "        if log_prob is not None:\n",
    "            self.buffer_log_probs.append(log_prob)\n",
    "        if reward is not None:\n",
    "            self.buffer_rewards.append(reward)\n",
    "        if value is not None:\n",
    "            self.buffer_values.append(value)\n",
    "        if terminal is not None:\n",
    "            self.buffer_terminals.append(terminal)\n",
    "        if entropy is not None:\n",
    "            self.buffer_entropies.append(entropy)\n",
    "        if hidden_state is not None:\n",
    "            self.buffer_hiddens.append(hidden_state)\n",
    "        \n",
    "    def clear_trajectory(self):\n",
    "        self.buffer_states = []\n",
    "        self.buffer_next_states = []\n",
    "        self.buffer_actions = []\n",
    "        self.buffer_rewards = []\n",
    "        self.buffer_terminals = []\n",
    "        self.buffer_log_probs = []\n",
    "     \n",
    "    \"\"\"@property\n",
    "    def states(self):\n",
    "        return torch.stack(self.buffer_states)\n",
    "    @property\n",
    "    def next_states(self):\n",
    "        return torch.stack(self.buffer_next_states)\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return torch.stack(self.buffer_actions)\n",
    "    @property\n",
    "    def log_probs(self):\n",
    "        return torch.stack(self.buffer_log_probs)\n",
    "    @property\n",
    "    def rewards(self):\n",
    "        return torch.stack(self.buffer_rewards)\n",
    "    @property\n",
    "    def values(self):\n",
    "        return torch.stack(self.buffer_values)\n",
    "    @property\n",
    "    def terminals(self):\n",
    "        return torch.stack(self.buffer_terminals)\n",
    "    @property\n",
    "    def entropies(self):\n",
    "        return torch.stack(self.buffer_entropies)\n",
    "    @property\n",
    "    def hidden_state(self):\n",
    "        return torch.stack(self.buffer_hiddens)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def update(self):\n",
    "        G = torch.zeros(len(self.buffer_states[0])).to(DEFAULT_DEVICE)\n",
    "        for t in reversed(range(len(self.buffer_states))):\n",
    "            G.mul_(self.discount_factor)\n",
    "            G.add_(self.buffer_rewards[t])\n",
    "            \n",
    "        log_probs = self.model(self.buffer_states[0])\n",
    "        action = self.buffer_actions[0]\n",
    "        performance = log_probs*G[:,None]\n",
    "        loss = torch.nn.NLLLoss(reduction = 'sum')(performance, action)\n",
    "        self.model_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.model_optim.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(Trajectory):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800, \n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01,\n",
    "                 lam = 10):\n",
    "        super().__init__()\n",
    "        self.model = NN(*NN_args)\n",
    "        self.model_optim = torch.optim.Adam(self.model.parameters(), lr=lr) #set learning rate\n",
    "        self.gamma = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model = torch.load(path)\n",
    "        self.model_optim = torch.optim.Adam(self.qnet.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "    def action(self, state):\n",
    "        action_probs, self.values = self.model(state)\n",
    "        dist = torch.distributions.Categorical(probs = action_probs)\n",
    "        actions = dist.sample()\n",
    "        self.log_probs = dist.log_prob(actions)\n",
    "        return actions\n",
    "\n",
    "    def update(self, next_states, rewards, terminals):\n",
    "        _, next_state_values = self.model(next_states)\n",
    "        returns = (rewards + (~terminals)*self.gamma*next_state_values).detach() \n",
    "        values = self.values\n",
    "        value_loss = torch.nn.functional.smooth_l1_loss(returns,values).mean()\n",
    "        advantages = returns-values\n",
    "\n",
    "        policy_loss = -(advantages.detach()*self.log_probs).mean()\n",
    "        loss = policy_loss + value_loss\n",
    "        self.model_optim.zero_grad()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.model_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeNet(torch.nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self.layer_length = size*size\n",
    "        self.common_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "        self.policy_layer = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 2, kernel_size=(1,1), stride=(1,1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(2*self.layer_length,4),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "        self.value_layer = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 1, kernel_size=(1,1), stride=(1,1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.layer_length,64),\n",
    "            torch.nn.Linear(64,1)\n",
    "            ).to(DEFAULT_DEVICE)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.common_layer(x)\n",
    "        self.policy = self.policy_layer(x)\n",
    "        self.value = self.value_layer(x)\n",
    "        return (self.policy, self.value.squeeze(-1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnakeNet(\n",
      "  (common_layer): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (policy_layer): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Flatten()\n",
      "    (3): Linear(in_features=200, out_features=4, bias=True)\n",
      "    (4): Softmax(dim=-1)\n",
      "  )\n",
      "  (value_layer): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Flatten()\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "environment = 'SingleSnake'\n",
    "num_envs = 1000 #Number of parallel environments to simulate. Use small value for cpu (eg. 1)\n",
    "test_num_envs = 100\n",
    "\n",
    "if environment == 'SimpleGridworld':\n",
    "    env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=True)\n",
    "    test_env = SimpleGridworld(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=Reinforce_Agent(NN = FNN_1, NN_args = (state_dim, 512, action_dim),\n",
    "                           lr=0.005, discount = 1.0)\n",
    "\n",
    "elif environment == 'SingleSnake':\n",
    "    env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset= True)\n",
    "    test_env = SingleSnake(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=A2C(NN = SnakeNet, NN_args = (10,), lr = 0.0005, discount =0.99)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid option\")\n",
    "\n",
    "#agent.load(\"models/best_model.h5\")\n",
    "agent.train()\n",
    "print(agent.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-2f6e0c3e4666>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#max steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mt_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mt_next_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_terminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;31m#anything with a positive reward is considered as food.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mfc_sum\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_reward\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Bath\\Reinforcement Learning\\Fast Snake\\wurm\\envs\\single_snake.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mfood_addition_env_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfood_removal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0madd_food_envs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfood_addition_env_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mfood_addition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_food_addition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_food_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfood_addition_env_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFOOD_CHANNEL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFOOD_CHANNEL\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfood_addition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Bath\\Reinforcement Learning\\Fast Snake\\wurm\\envs\\single_snake.py\u001b[0m in \u001b[0;36m_get_food_addition\u001b[1;34m(self, envs)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mavailable_locations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mfood_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavailable_locations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         food_addition = torch.sparse_coo_tensor(\n\u001b[0;32m    352\u001b[0m             food_indices.t(),  torch.ones(len(food_indices)), available_locations.shape, device=self.device)\n",
      "\u001b[1;32m~\\OneDrive - University of Bath\\Reinforcement Learning\\Fast Snake\\wurm\\utils.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[1;34m(tensor, column, random)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0muniq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[0munique\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Bath\\Reinforcement Learning\\Fast Snake\\wurm\\utils.py\u001b[0m in \u001b[0;36munique1d\u001b[1;34m(tensor, return_index)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "render=False\n",
    "save_model = False\n",
    "number_of_steps = 100000\n",
    "epsilon = 1.0\n",
    "####Code to compute total reward####\n",
    "\n",
    "total_reward = torch.zeros(num_envs).to(DEFAULT_DEVICE)\n",
    "step_list=[]\n",
    "fc_list=[] #food collected\n",
    "best_fc = 0\n",
    "####Code to compute total reward####\n",
    "\n",
    "\n",
    "agent.train()\n",
    "\n",
    "state=env.reset()\n",
    "#Learning\n",
    "for i in range(0,number_of_episodes):\n",
    "    ##############Learning######################\n",
    "\n",
    "    action = agent.action(state) \n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.update(next_state, reward, terminal)  \n",
    "    state = next_state\n",
    "\n",
    "    #############Validation############################\n",
    "    if i%10 == 0:\n",
    "        agent.evaluate()                        \n",
    "        t_state = test_env.reset()\n",
    "        fc_sum = torch.zeros((test_num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "        #hit_terminal = torch.zeros((test_num_envs,)).bool().to(DEFAULT_DEVICE)\n",
    "        for _ in range(200): #max steps\n",
    "            t_action = agent.action(t_state)\n",
    "            t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "            #anything with a positive reward is considered as food.\n",
    "            fc_sum+=(t_reward>0).float()\n",
    "            #hit_terminal |= t_terminal\n",
    "            t_state = t_next_state\n",
    "            if t_terminal.all():\n",
    "                break\n",
    "\n",
    "        t_sum = fc_sum.cpu().numpy()\n",
    "        t_mean = np.mean(t_sum)\n",
    "        print('Step:', i)\n",
    "        print(\"Episode Completed:\", t_terminal.sum().cpu().numpy(), \"/\", test_num_envs)\n",
    "        print(\"Mean, Median, Max, Min, std:\", \n",
    "              t_mean, \n",
    "              np.median(t_sum),\n",
    "              np.max(t_sum),\n",
    "              np.min(t_sum),\n",
    "              np.std(t_sum))\n",
    "        fc_list.append(t_mean)\n",
    "        step_list.append(i)\n",
    "        plt.plot(step_list, fc_list)\n",
    "        plt.show()\n",
    "        agent.train()\n",
    "        clear_output(wait=True)\n",
    "        if t_mean>best_fc:\n",
    "            best_fc = t_mean\n",
    "            #torch.save(agent.model,\"models/best_model.h5\")\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward>),\n",
       " tensor([-0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681,\n",
       "         -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681, -0.8681],\n",
       "        device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 1, 1, 3, 1, 2, 0, 3, 0, 3, 1, 1, 2, 2, 1, 1, 1, 0, 2, 3, 0,\n",
       "        3, 1, 1, 3, 3, 1, 1, 1, 3, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 1, 0, 0, 2, 0,\n",
       "        3, 0, 1, 1, 2, 3, 0, 1, 3, 0, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 3, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 1, 3, 3, 1, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 3, 1, 1, 1, 0, 3, 0, 0, 1,\n",
       "        1, 3, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 3, 0, 1, 3, 3, 1, 1, 1, 3, 3,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 3, 1, 0, 0, 1, 0, 1, 1, 1, 2, 3, 3, 0, 3,\n",
       "        1, 1, 2, 0, 1, 1, 2, 0, 1, 1, 2, 1, 2, 1, 2, 3, 3, 1, 0, 3, 2, 2, 3, 1,\n",
       "        1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 3, 2, 0, 1, 1, 1, 1, 0, 3, 0, 1, 1, 2, 1,\n",
       "        2, 2, 1, 0, 0, 3, 1, 3, 1, 3, 3, 3, 1, 0, 1, 3, 1, 1, 3, 0, 0, 1, 2, 0,\n",
       "        3, 1, 1, 1, 3, 1, 0, 1, 0, 3, 1, 3, 2, 0, 1, 3, 3, 2, 1, 1, 2, 3, 2, 1,\n",
       "        0, 1, 2, 1, 0, 0, 1, 0, 3, 0, 1, 0, 3, 0, 1, 3, 0, 0, 2, 1, 1, 0, 1, 2,\n",
       "        1, 1, 1, 0, 2, 2, 2, 3, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1,\n",
       "        3, 1, 1, 1, 1, 0, 3, 1, 1, 0, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 3, 3, 1, 2, 1, 3, 1, 2, 1, 2, 2, 1, 1, 0, 1, 1, 3, 0, 3, 1, 1,\n",
       "        3, 3, 1, 3, 0, 1, 1, 2, 3, 3, 1, 3, 3, 0, 3, 3, 1, 2, 0, 1, 1, 3, 2, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 3, 2, 2, 3, 1, 3, 3, 3, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 2, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0,\n",
       "        0, 1, 2, 3, 1, 1, 2, 0, 2, 0, 1, 0, 1, 1, 2, 0, 1, 2, 1, 2, 1, 1, 1, 2,\n",
       "        1, 3, 1, 0, 1, 2, 0, 1, 0, 1, 1, 3, 0, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0,\n",
       "        1, 0, 3, 3, 1, 0, 1, 2, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 1, 2, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 3, 0, 3, 1, 3, 1, 1, 2, 2, 2, 3, 1, 1,\n",
       "        2, 2, 3, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 3, 3, 3, 0, 1,\n",
       "        2, 1, 3, 1, 2, 0, 1, 1, 1, 2, 2, 0, 3, 2, 2, 3, 0, 0, 3, 0, 0, 3, 3, 0,\n",
       "        1, 1, 1, 0, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 2, 2, 0, 3, 0, 2, 0, 1, 0, 1, 2, 0, 1, 1, 2, 0, 2, 3, 0, 1,\n",
       "        3, 3, 1, 0, 1, 3, 1, 1, 2, 2, 2, 1, 1, 3, 1, 3, 2, 1, 2, 3, 1, 0, 1, 3,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 3, 1, 1, 1, 1, 3, 0, 1, 3, 3, 0, 0, 1, 1,\n",
       "        1, 3, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 2, 3, 0, 0,\n",
       "        1, 3, 1, 1, 1, 2, 3, 1, 3, 1, 1, 1, 1, 0, 3, 0, 1, 2, 2, 1, 1, 1, 2, 2,\n",
       "        2, 1, 3, 0, 3, 2, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 0, 3, 1, 2, 1, 1, 1, 0,\n",
       "        0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 2, 1, 1, 2, 2, 1, 3, 1, 1, 1, 3, 2, 1, 1,\n",
       "        2, 2, 2, 0, 3, 0, 1, 1, 3, 1, 1, 0, 0, 3, 2, 2, 1, 1, 1, 2, 1, 3, 1, 1,\n",
       "        0, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1, 3, 3, 1, 1, 0, 3, 2, 1, 0, 0, 1, 1, 2,\n",
       "        1, 1, 1, 1, 3, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 3, 1, 2, 3,\n",
       "        1, 1, 2, 0, 1, 1, 0, 0, 2, 1, 0, 3, 0, 0, 1, 3, 1, 2, 1, 1, 0, 0, 1, 2,\n",
       "        3, 3, 1, 2, 2, 0, 0, 1, 2, 3, 3, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 2, 1, 1,\n",
       "        2, 3, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 2,\n",
       "        3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 2, 0, 1, 1, 0, 2, 2, 1, 2, 2, 1, 0, 0,\n",
       "        1, 0, 2, 2, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 0, 3, 0, 1, 3, 1, 1, 1, 0, 2,\n",
       "        1, 2, 0, 1, 1, 3, 0, 1, 1, 1, 2, 2, 1, 1, 1, 3, 3, 3, 1, 1, 2, 1, 0, 3,\n",
       "        3, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 1, 3, 3, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "          [-1.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1.,  0.,  0.,  1.,  2.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "          [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]]],\n",
      "       device='cuda:0')\n",
      "(tensor([[0., 1., 0., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward>), tensor([-0.8058], device='cuda:0', grad_fn=<SqueezeBackward1>))\n",
      "tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(state)\n",
    "print(agent.model(state))\n",
    "action = agent.action(state) \n",
    "print(action)\n",
    "next_state, reward, terminal, _ = env.step(action)\n",
    "agent.update(next_state, reward, terminal)  \n",
    "state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Categorical(probs = p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = agent.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
