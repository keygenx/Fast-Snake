{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from wurm.envs import SingleSnake\n",
    "DEFAULT_DEVICE = 'cuda' #set device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "state = env.reset()\n",
    "state_dim = state.shape[1:]\n",
    "action_dim = 4\n",
    "qnet=Q_Network(state_dim, 4, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size: int):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer = collections.deque(maxlen=max_buffer_size)\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    #Sample batches and sub sample parallel environments\n",
    "    def sample_minibatch(self,minibatch_length, subbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        sub_length = self.buffer[0][0].shape[0]\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), minibatch_length):\n",
    "            rand_int_1 = np.random.randint(0, sub_length, subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0][rand_int_1])\n",
    "            next_states.append(transition[1][rand_int_1])\n",
    "            actions.append(transition[2][rand_int_1])\n",
    "            rewards.append(transition[3][rand_int_1])\n",
    "            terminals.append(transition[4][rand_int_1])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "    \n",
    "    #sample parallel environments from a randomly selected memory.\n",
    "    def sample(self, subbatch_length):\n",
    "            rand_int = np.random.randint(0, len(self.buffer))\n",
    "            rand_int_1 = np.random.randint(0, len(self.buffer[0][0]), subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states=transition[0][rand_int_1]\n",
    "            next_states=transition[1][rand_int_1]\n",
    "            actions=transition[2][rand_int_1]\n",
    "            rewards=transition[3][rand_int_1]\n",
    "            terminals=transition[4][rand_int_1]\n",
    "            return (states,next_states,actions,rewards,terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, num_envs: int, buffer_size: int, NN: object, *NN_args):\n",
    "        #self.qnet = torch.load(\"dqn80x80.h5\")\n",
    "        self.qnet = NN(*NN_args)\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "        self.qnet_optim = torch.optim.Adam( self.qnet.parameters(), lr=0.0005) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([0.8]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = nn.SmoothL1Loss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size) #set size of replay buffer.\n",
    "        self.target_update_interval = 1 #set for target update interval for hard target network updates\n",
    "        self.update_count=0\n",
    "        self.tau = 0.1 # set tau for soft target network updates\n",
    "        self.num_envs = num_envs\n",
    "        pass\n",
    "    \n",
    "    def add_to_buffer(self, data):\n",
    "        self.replay_buffer.add_to_buffer(data)\n",
    "        \n",
    "#Update target network\n",
    "    def target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data)\n",
    "     \n",
    "#Soft update target network\n",
    "    def soft_target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data*self.tau + (1-self.tau)*target_net_params)\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.qnet(state), dim=1)  # choose greedy action\n",
    "    \n",
    "#update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        qsa_next_action = self.qnet_target(next_state)\n",
    "        qsa_next_action = torch.max(qsa_next_action, dim=1)[0]\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "     \n",
    "#call this to update Q network (train) and then make hard update of target network\n",
    "    def update(self, update_rate):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(self.num_envs//3)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.update_count+=1\n",
    "            if self.update_count==self.target_update_interval:\n",
    "                self.target_update(self.qnet, self.qnet_target)\n",
    "                self.update_count=0\n",
    "                \n",
    "#call this to update Q network (train) and then make soft update of target network\n",
    "    def soft_update(self, update_rate):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(self.num_envs//3)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.qnet, self.qnet_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fully connected neural network\n",
    "def FNN_1(shape, action_dim, hidden_dim):\n",
    "    flat_shape = np.product(shape) #length of the flattened state\n",
    "    model = nn.Sequential(\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(flat_shape,hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        #nn.Linear(hidden_dim,hidden_dim),\n",
    "                        #nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, action_dim),\n",
    "                         ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_1():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(32, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_2():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(32, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  (9): Flatten()\n",
       "  (10): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_envs = 1300 #Number of parallel environments to simulate. Use small value for cpu (eg. 1)\n",
    "env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "\n",
    "state = env.reset()\n",
    "state_dim = state.shape[1:]\n",
    "action_dim = 4\n",
    "\n",
    "agent=DQNAgent(num_envs = num_envs, buffer_size = 100, NN = CNN_2)\n",
    "agent.qnet.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 294\n",
      "\n",
      "avg_reward: 0.75757575 \n",
      "avg_moves: 14.787879\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "render=False\n",
    "save_model = True\n",
    "number_of_episodes = 2000\n",
    "\n",
    "reward_sum=torch.zeros([num_envs]).to(DEFAULT_DEVICE)\n",
    "total_moves = torch.zeros([num_envs]).to(DEFAULT_DEVICE)\n",
    "list_reward =[]\n",
    "\n",
    "state=env.reset()\n",
    "agent.qnet.train()\n",
    "\n",
    "\n",
    "for i in range(1,number_of_episodes):\n",
    "    action = agent.epsilon_greedy_action( state , 0.1) #set epsilon\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    \n",
    "    agent.add_to_buffer((state,next_state,action,reward,terminal))\n",
    "    \n",
    "    agent.soft_update(40)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    reward_sum.add_(reward)\n",
    "    total_moves.add_(1)\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "        \n",
    "    if terminal.any():\n",
    "        print('episode:', i)\n",
    "        print('\\navg_reward:', np.mean(reward_sum[terminal].cpu().numpy()),'\\navg_moves:', np.mean(total_moves[terminal].cpu().numpy()))       \n",
    "        clear_output(wait=True)\n",
    "        list_reward.append(np.mean(reward_sum[terminal].cpu().numpy()))\n",
    "        reward_sum[terminal]=0\n",
    "        total_moves[terminal]=0\n",
    "    \n",
    "if render:    \n",
    "    env.close()\n",
    "    \n",
    "if save_model:\n",
    "    torch.save(agent.qnet,\"save_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reward_1 = [np.mean(list_reward[i-100:i]) for i in range(100, len(list_reward))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(reward_1)\n",
    "plt.plot(reward_2)\n",
    "plt.plot(reward_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.8948, 3.9775, 3.3580, 7.7497]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[4.9992, 5.8710, 4.6602, 9.2249]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[5.0585, 6.5257, 6.4130, 5.4418]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[3.6331, 5.6519, 4.9770, 4.1272]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8641,  3.9909,  3.9817, -8.8551]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([-10.], device='cuda:0')\n",
      "episode: 0 sum_of_rewards_for_episode: [-5.]\n",
      "tensor([[3.3407, 5.9662, 3.8319, 4.9328]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[2.3820, 4.7629, 2.8067, 4.0156]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[2.3867, 3.8322, 2.2346, 2.7260]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.2927,  2.5089,  1.8961, -9.6555]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([-10.], device='cuda:0')\n",
      "episode: 1 sum_of_rewards_for_episode: [-10.]\n",
      "tensor([[8.2982, 4.7599, 5.4088, 5.6690]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[3.8827, 2.7397, 2.4080, 3.4893]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[4.6315, 4.1209, 3.1056, 5.1233]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[6.4724, 4.4164, 3.3186, 3.8942]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[-6.1894,  0.5387,  0.7938, -4.0779]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([-10.], device='cuda:0')\n",
      "episode: 2 sum_of_rewards_for_episode: [0.]\n",
      "tensor([[2.9812, 1.8743, 2.1520, 4.5241]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[1.8280, 0.6497, 1.8659, 3.6568]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7599, -0.7575,  0.8003,  1.9502]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([-10.], device='cuda:0')\n",
      "episode: 3 sum_of_rewards_for_episode: [-10.]\n",
      "tensor([[5.4693, 2.8785, 3.7989, 5.4739]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[3.5885, 1.6884, 2.6820, 4.0956]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[1.9613, 0.0573, 1.6200, 3.0733]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([-10.], device='cuda:0')\n",
      "episode: 4 sum_of_rewards_for_episode: [-10.]\n"
     ]
    }
   ],
   "source": [
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "agent.qnet.eval()\n",
    "state = env.reset()\n",
    "for episode in range(5):\n",
    "    reward_sum = 0\n",
    "\n",
    "\n",
    "    while(1):\n",
    "        action = agent.epsilon_greedy_action( state , 0.0)\n",
    "        #print(agent.qnet(state))\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        reward_sum+= reward.cpu().numpy()\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        state = next_state\n",
    "        if terminal:\n",
    "            print(reward)\n",
    "            break\n",
    "    print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
