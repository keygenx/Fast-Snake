{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os #to get current working directory\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle #for storing data\n",
    "from wurm.envs import SingleSnake\n",
    "from wurm.envs import SimpleGridworld\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "DEFAULT_DEVICE = 'cuda' #set device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size: int):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer = collections.deque(maxlen=max_buffer_size)\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    #Sample batches and sub sample parallel environments\n",
    "    def sample_minibatch(self,minibatch_length, subbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        sub_length = self.buffer[0][0].shape[0]\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), minibatch_length):\n",
    "            rand_int_1 = np.random.randint(0, sub_length, subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0][rand_int_1])\n",
    "            next_states.append(transition[1][rand_int_1])\n",
    "            actions.append(transition[2][rand_int_1])\n",
    "            rewards.append(transition[3][rand_int_1])\n",
    "            terminals.append(transition[4][rand_int_1])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "\n",
    "    def sample_batch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), minibatch_length):\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "    \n",
    "    #sample parallel environments from a randomly selected memory.\n",
    "    def sample(self, subbatch_length):\n",
    "            rand_int = np.random.randint(0, len(self.buffer))\n",
    "            rand_int_1 = np.random.randint(0, len(self.buffer[0][0]), subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states=transition[0][rand_int_1]\n",
    "            next_states=transition[1][rand_int_1]\n",
    "            actions=transition[2][rand_int_1]\n",
    "            rewards=transition[3][rand_int_1]\n",
    "            terminals=transition[4][rand_int_1]\n",
    "            return (states,next_states,actions,rewards,terminals)\n",
    "\n",
    "        \n",
    "#A buffer with lesser correlation between samples. Implemented with pytorch. \n",
    "#Presently not working properly. Not sure why.\n",
    "class BetterBuffer():\n",
    "    def __init__(self, max_envs: int = 1000):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer_0 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_1 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_2 = torch.empty(0).long().to(DEFAULT_DEVICE)\n",
    "        self.buffer_3 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_4 = torch.empty(0).bool().to(DEFAULT_DEVICE)\n",
    "        self.max_length = max_envs\n",
    "        self.pointer = 0\n",
    "        self.full = False\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        if self.full == True:\n",
    "            if self.pointer==self.max_length:\n",
    "                self.pointer=0\n",
    "            self.buffer_0[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[0]\n",
    "            self.buffer_1[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[1]\n",
    "            self.buffer_2[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[2]\n",
    "            self.buffer_3[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[3]\n",
    "            self.buffer_4[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[4]\n",
    "            self.pointer+=1\n",
    "        else:\n",
    "            self.buffer_0=torch.cat((self.buffer_0,data[0]))\n",
    "            self.buffer_1=torch.cat((self.buffer_1,data[1]))\n",
    "            self.buffer_2=torch.cat((self.buffer_2,data[2]))\n",
    "            self.buffer_3=torch.cat((self.buffer_3,data[3]))\n",
    "            self.buffer_4=torch.cat((self.buffer_4,data[4]))\n",
    "            self.pointer+=1\n",
    "            if self.pointer==self.max_length:\n",
    "                self.full=True\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if self.full == True:\n",
    "            randint = torch.randint(0, self.max_length*num_envs,(batch_size,))\n",
    "        else:\n",
    "            randint = torch.randint(0,self.pointer*num_envs, (batch_size,))\n",
    "        return self.buffer_0[randint], self.buffer_1[randint], self.buffer_2[randint], self.buffer_3[randint], self.buffer_4[randint]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################Simple DQN Agent########################################\n",
    "class DQNAgent():\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800, \n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01):\n",
    "        #self.qnet = torch.load(\"dqn80x80.h5\")\n",
    "        self.qnet = NN(*NN_args)\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "        self.qnet_optim = torch.optim.Adam( self.qnet.parameters(), lr=lr) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = torch.nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size) #set size of replay buffer.\n",
    "        self.old_buffer = ReplayBuffer(300)\n",
    "        \n",
    "        self.target_update_interval = 500 #set for target update interval for hard target network updates\n",
    "        self.update_count= 0 #internal working variable. Don't change\n",
    "        self.tau = tau # set tau for soft target network updates\n",
    "        self.num_envs = num_envs\n",
    "        pass\n",
    "    \n",
    "    def add_to_buffer(self, data):\n",
    "        self.replay_buffer.add_to_buffer(data)\n",
    "    def add_to_old_buffer(self, data):\n",
    "        self.old_buffer.add_to_buffer(data)\n",
    "    \n",
    "    def train(self):\n",
    "        self.qnet.train()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.qnet.eval()\n",
    "        \n",
    "    #Hard update target network\n",
    "    def target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data)\n",
    "     \n",
    "    #Soft update target network\n",
    "    def soft_target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data*self.tau + (1-self.tau)*target_net_params)\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.qnet(state), dim=1)  # choose greedy action\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        qsa_next_action = self.qnet_target(next_state)\n",
    "        qsa_next_action = torch.max(qsa_next_action, dim=1)[0]\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "     \n",
    "    #call this to update Q network (train) and then make hard update of target network\n",
    "    def update(self, update_rate):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(self.num_envs//3)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.update_count+=1\n",
    "            if self.update_count==self.target_update_interval:\n",
    "                self.target_update(self.qnet, self.qnet_target)\n",
    "                self.update_count=0\n",
    "                \n",
    "    #call this to update Q network (train) and then make soft update of target network\n",
    "    def soft_update(self, update_rate, batch_size):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(batch_size)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.qnet, self.qnet_target)\n",
    "\n",
    "###############Simple DQN Agent######################################################            \n",
    "#################Double DQN Agent smooth##############################################\n",
    "#Based on https://arxiv.org/abs/1509.06461v3\n",
    "class DDQNAgent_smooth(DQNAgent):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800,\n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01):\n",
    "        super().__init__(NN, NN_args, num_envs, buffer_size, lr, discount, tau)\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        q_target_next_state_a = self.qnet_target(next_state)\n",
    "        q_target_next_state_max_a = torch.argmax(q_target_next_state_a, dim=1)\n",
    "        q_next_state_a = torch.gather(self.qnet(next_state), dim=1, index=q_target_next_state_max_a.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * q_next_state_a\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    #call this to update Q network (train) and then make soft update of target network\n",
    "    def update(self, update_rate, batch_size, rehearse_rate = 0):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(batch_size)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.qnet, self.qnet_target)\n",
    "        for _ in range(rehearse_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.old_buffer.sample(batch_size)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.qnet, self.qnet_target)\n",
    "#################Double DQN Agent smooth##################################\n",
    "\n",
    "#################Double DQN Agent########################################\n",
    "#Based on https://arxiv.org/abs/1509.06461v1\n",
    "class DDQNAgent(DQNAgent):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800,\n",
    "                 lr: float = 0.0005, discount: float = 0.8):\n",
    "        #self.qnet = torch.load(\"dqn80x80.h5\")\n",
    "        self.Q_A = NN(*NN_args)\n",
    "        self.Q_B = NN(*NN_args)\n",
    "        self.Q_A_optim = torch.optim.Adam( self.Q_A.parameters(), lr=lr) #set learning rate\n",
    "        self.Q_B_optim = torch.optim.Adam( self.Q_B.parameters(), lr=lr) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = torch.nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size) #set size of replay buffer.\n",
    "        self.num_envs = num_envs\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        self.Q_A.train()\n",
    "        self.Q_A.train()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.Q_A.eval()\n",
    "        self.Q_B.eval()\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.Q_A(state), dim=1)  # choose greedy action\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_A_Network(self, state, next_state, action, reward, terminals):\n",
    "        QA_s_a = torch.gather(self.Q_A(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        QA_sn_a = self.Q_A(next_state)\n",
    "        QA_sn_a_max = torch.argmax(QA_sn_a, dim=1)\n",
    "        QB_sn_a = torch.gather(self.Q_B(next_state), dim=1, index=QA_sn_a_max.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        QA_s_a_target = reward + not_terminals * self.discount_factor * QB_sn_a\n",
    "        q_network_loss = self.MSELoss_function(QA_s_a, QA_s_a_target.detach())\n",
    "        self.Q_A_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.Q_A_optim.step()\n",
    "        \n",
    "    def update_Q_B_Network(self, state, next_state, action, reward, terminals):\n",
    "        QB_s_a = torch.gather(self.Q_B(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        QB_sn_a = self.Q_B(next_state)\n",
    "        QB_sn_a_max = torch.argmax(QB_sn_a, dim=1)\n",
    "        QA_sn_a = torch.gather(self.Q_A(next_state), dim=1, index=QB_sn_a_max.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        QB_s_a_target = reward + not_terminals * self.discount_factor * QA_sn_a\n",
    "        q_network_loss = self.MSELoss_function(QB_s_a, QB_s_a_target.detach())\n",
    "        self.Q_B_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.Q_B_optim.step()\n",
    "        \n",
    "    #call this to update Q network (train) and then make soft update of target network\n",
    "    def update(self, update_rate, batch_size):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(batch_size)\n",
    "            if np.random.uniform()<0.5:\n",
    "                self.update_Q_A_Network(states, next_states, actions, rewards, terminals)\n",
    "            else:\n",
    "                self.update_Q_B_Network(states, next_states, actions, rewards, terminals)\n",
    "#################Double DQN Agent##############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fully connected neural network\n",
    "def FNN_1(shape, hidden_dim, action_dim):\n",
    "    flat_shape = np.product(shape) #length of the flattened state\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(flat_shape,hidden_dim),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim, action_dim),\n",
    "         ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_1():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_2():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(32, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_3():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(64, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=100, out_features=512, bias=True)\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (5): Dropout(p=0.2, inplace=False)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "environment = 'SimpleGridworld'\n",
    "num_envs = 1300 #Number of parallel environments to simulate. Use small value for cpu (eg. 1)\n",
    "test_num_envs = 100\n",
    "\n",
    "if environment == 'SimpleGridworld':\n",
    "    env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "    test_env = SimpleGridworld(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=DDQNAgent_smooth(NN = FNN_1, NN_args = (state_dim, 512, action_dim),\n",
    "                           num_envs = num_envs, buffer_size = 600, lr = 0.0005,\n",
    "                           discount = 0.8, tau =0.01)\n",
    "\n",
    "\n",
    "elif environment == 'SimpleSnake':\n",
    "    env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "    test_env = SingleSnake(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=DDQNAgent_smooth(NN = CNN_1, num_envs = num_envs, buffer_size = 600, lr = 0.0005, discount = 0.8, tau =0.01)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid option\")\n",
    "    \n",
    "agent.train()\n",
    "print(agent.qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 800 Mean, Median, Max, Min, std: 15.97 11.0 110.0 0.0 19.192423\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "render=False\n",
    "save_model = False\n",
    "number_of_episodes = 100000\n",
    "epsilon = 1.0\n",
    "####Code to compute total reward####\n",
    "\n",
    "total_reward = torch.zeros(num_envs).to(DEFAULT_DEVICE)\n",
    "episode_list=[]\n",
    "fc_list=[] #food collected\n",
    "best_fc = 0\n",
    "####Code to compute total reward####\n",
    "\n",
    "state=env.reset()\n",
    "agent.train()\n",
    "\n",
    "#Filling up the buffer\n",
    "for i in range(30):\n",
    "    action = agent.epsilon_greedy_action( state , 1.0) #set epsilon\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.add_to_buffer((state,next_state,action,reward,terminal))\n",
    "    state = next_state\n",
    "\n",
    "\n",
    "#Learning\n",
    "for i in range(1,number_of_episodes):\n",
    "    action = agent.epsilon_greedy_action( state , epsilon) #set epsilon\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    \n",
    "    agent.add_to_buffer((state,next_state,action,reward,terminal))\n",
    "    \n",
    "    agent.update(10,400)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if i==500:\n",
    "        epsilon = 0.1\n",
    "    elif i==3000:\n",
    "        epsilon = 0.01\n",
    "    elif i==10000:\n",
    "        epsilon = 0.001\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    #New code for recording data. Slower but more accurate.\n",
    "    if (i%100==0):\n",
    "        agent.evaluate()                        \n",
    "        t_state = test_env.reset()\n",
    "        fc_sum = torch.zeros((test_num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "        #hit_terminal = torch.zeros((test_num_envs,)).bool().to(DEFAULT_DEVICE)\n",
    "\n",
    "        for steps in range(1000): #max steps\n",
    "            t_action = agent.epsilon_greedy_action( t_state , 0.0)\n",
    "            t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "            #anything with a positive reward is considered as food.\n",
    "            fc_sum+=(t_reward>0).float()\n",
    "            #hit_terminal |= t_terminal\n",
    "            t_state = t_next_state\n",
    "            if t_terminal.all():\n",
    "                break\n",
    "\n",
    "        t_sum = fc_sum.cpu().numpy()\n",
    "        t_mean = np.mean(t_sum)\n",
    "        print('episode:', i, \"Mean, Median, Max, Min, std:\", \n",
    "              t_mean, \n",
    "              np.median(t_sum),\n",
    "              np.max(t_sum),\n",
    "              np.min(t_sum),\n",
    "              np.std(t_sum))\n",
    "        fc_list.append(t_mean)\n",
    "        episode_list.append(i)\n",
    "        plt.plot(episode_list, fc_list)\n",
    "        plt.show()\n",
    "        agent.train()\n",
    "        clear_output(wait=True)\n",
    "        if t_mean>best_fc:\n",
    "            best_fc = t_mean\n",
    "            torch.save(agent.qnet,\"models/best_model.h5\")\n",
    "    \n",
    "if render:    \n",
    "    env.close()\n",
    "    \n",
    "if save_model:\n",
    "    model = torch.load(\"best_model.h5\")\n",
    "    with open('models/best_model.pickle', 'wb') as f:\n",
    "        pickle.dump((model, episode_list,reward_list), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data of best model and associated runtime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualizing reward\n",
    "plt.plot(episode_list, fc_list)\n",
    "plt.show()\n",
    "\n",
    "#Store data about the best model as a pickle file\n",
    "import pickle\n",
    "model = torch.load(\"models/best_model.h5\")\n",
    "with open('models/cnn_256_avg_822.pickle', 'xb') as f:\n",
    "    pickle.dump((model, episode_list,reward_list), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the saved model\n",
    "with open('models/cnn_256_avg_8.5.pickle', 'rb') as f:\n",
    "     data = pickle.load(f)\n",
    "\n",
    "model = data[0].to('cuda')        \n",
    "#agent.qnet= data[0]\n",
    "\n",
    "#plt.plot(data[1], data[2])\n",
    "#plt.show()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Record Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = SimpleGridworld(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "agent.evaluate()\n",
    "PATH = os.getcwd()\n",
    "state = env.reset()\n",
    "for episode in range(1):\n",
    "    fc_sum = 0\n",
    "    recorder = VideoRecorder(env, path=PATH + f'/videos/{episode}.mp4')\n",
    "    env.render()\n",
    "    recorder.capture_frame()\n",
    "    time.sleep(0.2)\n",
    "    counter = 0\n",
    "    while(1):\n",
    "        counter+=1\n",
    "        action = agent.epsilon_greedy_action( state , 0.0)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        fc_sum+= (reward>0).cpu().numpy()\n",
    "        env.render()\n",
    "        recorder.capture_frame()\n",
    "        #time.sleep(0.2)\n",
    "        state = next_state\n",
    "        if terminal.all() or counter==1000:\n",
    "            recorder.close()\n",
    "            break\n",
    "    print('episode:', episode, 'Food Collected:', fc_sum)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Average Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 0\n",
      "Mean, Median, Max, Min, std: 66.88077 49.0 204.0 0.0 57.319176\n"
     ]
    }
   ],
   "source": [
    "test_env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "agent.evaluate()\n",
    "\n",
    "                       \n",
    "t_state = test_env.reset()\n",
    "fc_sum = torch.zeros((num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "\n",
    "for steps in range(1000): #max steps\n",
    "    t_action = agent.epsilon_greedy_action( t_state , 0.0)\n",
    "    t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "    #anything with a positive reward is considered as food.\n",
    "    fc_sum+=(t_reward>0).float()\n",
    "    t_state = t_next_state\n",
    "    if t_terminal.all():\n",
    "        break\n",
    "\n",
    "t_sum = fc_sum.cpu().numpy()\n",
    "t_mean = np.mean(t_sum)\n",
    "print(\"Completed:\", t_terminal.sum().cpu().numpy())\n",
    "print(\"Mean, Median, Max, Min, std:\", \n",
    "      t_mean, \n",
    "      np.median(t_sum),\n",
    "      np.max(t_sum),\n",
    "      np.min(t_sum),\n",
    "      np.std(t_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(env.random_action())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
