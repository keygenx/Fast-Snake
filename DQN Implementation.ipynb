{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os #to get current working directory\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle #for storing data\n",
    "from wurm.envs import SingleSnake\n",
    "from wurm.envs import SimpleGridworld\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "DEFAULT_DEVICE = 'cuda' #set device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size: int):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer = collections.deque(maxlen=max_buffer_size)\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    #Sample superbatches and sub sample parallel environments\n",
    "    def sample_subbatch(self,superbatch_length, subbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        sub_length = self.buffer[0][0].shape[0]\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), superbatch_length):\n",
    "            rand_int_1 = np.random.randint(0, sub_length, subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0][rand_int_1])\n",
    "            next_states.append(transition[1][rand_int_1])\n",
    "            actions.append(transition[2][rand_int_1])\n",
    "            rewards.append(transition[3][rand_int_1])\n",
    "            terminals.append(transition[4][rand_int_1])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "\n",
    "    def sample_superbatch(self,superbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), superbatch_length):\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "    \n",
    "    #sample parallel environments of subbatch_length from a randomly selected buffer location.\n",
    "    def sample(self, subbatch_length):\n",
    "            rand_int = np.random.randint(0, len(self.buffer))\n",
    "            rand_int_1 = np.random.randint(0, len(self.buffer[0][0]), subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states=transition[0][rand_int_1]\n",
    "            next_states=transition[1][rand_int_1]\n",
    "            actions=transition[2][rand_int_1]\n",
    "            rewards=transition[3][rand_int_1]\n",
    "            terminals=transition[4][rand_int_1]\n",
    "            return (states,next_states,actions,rewards,terminals)\n",
    "\n",
    "        \n",
    "#A buffer with lesser correlation between samples. Implemented with pytorch. \n",
    "#Presently not working properly. Not sure why.\n",
    "class BetterBuffer():\n",
    "    def __init__(self, max_envs: int = 1000):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer_0 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_1 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_2 = torch.empty(0).long().to(DEFAULT_DEVICE)\n",
    "        self.buffer_3 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_4 = torch.empty(0).bool().to(DEFAULT_DEVICE)\n",
    "        self.max_length = max_envs\n",
    "        self.pointer = 0\n",
    "        self.full = False\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        if self.full == True:\n",
    "            if self.pointer==self.max_length:\n",
    "                self.pointer=0\n",
    "            self.buffer_0[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[0]\n",
    "            self.buffer_1[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[1]\n",
    "            self.buffer_2[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[2]\n",
    "            self.buffer_3[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[3]\n",
    "            self.buffer_4[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[4]\n",
    "            self.pointer+=1\n",
    "        else:\n",
    "            self.buffer_0=torch.cat((self.buffer_0,data[0]))\n",
    "            self.buffer_1=torch.cat((self.buffer_1,data[1]))\n",
    "            self.buffer_2=torch.cat((self.buffer_2,data[2]))\n",
    "            self.buffer_3=torch.cat((self.buffer_3,data[3]))\n",
    "            self.buffer_4=torch.cat((self.buffer_4,data[4]))\n",
    "            self.pointer+=1\n",
    "            if self.pointer==self.max_length:\n",
    "                self.full=True\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if self.full == True:\n",
    "            randint = torch.randint(0, self.max_length*num_envs,(batch_size,))\n",
    "        else:\n",
    "            randint = torch.randint(0,self.pointer*num_envs, (batch_size,))\n",
    "        return self.buffer_0[randint], self.buffer_1[randint], self.buffer_2[randint], self.buffer_3[randint], self.buffer_4[randint]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################Simple DQN Agent########################################\n",
    "class DQNAgent():\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800, \n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01,\n",
    "                 lam = 10):\n",
    "        #self.qnet = torch.load(\"dqn80x80.h5\")\n",
    "        self.qnet = NN(*NN_args)\n",
    "        \n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "        for param in self.qnet_target.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        self.qnet_optim = torch.optim.Adam( self.qnet.parameters(), lr=lr) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = torch.nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size) #set size of replay buffer.\n",
    "        self.old_buffer = ReplayBuffer(300)\n",
    "        \n",
    "        self.target_update_interval = 500 #set for target update interval for hard target network updates\n",
    "        self.update_count= 0 #internal working variable. Don't change\n",
    "        self.tau = tau # set tau for soft target network updates\n",
    "        self.num_envs = num_envs\n",
    "        \n",
    "        self.ewc_counter = 0 #flag keep track of fisher matrix computations.\n",
    "        self.lam = torch.Tensor([lam]).to(DEFAULT_DEVICE)\n",
    "        self.ewc_on = False\n",
    "        self.fisher_recompute_interval = 3000\n",
    "        \n",
    "    def add_to_buffer(self, data):\n",
    "        self.replay_buffer.add_to_buffer(data)\n",
    "    \n",
    "    #computing diagonal of fisher matrix\n",
    "    def compute_fisher_diagonal(self, data):\n",
    "        print(\"Computing Fisher diagonal...\")\n",
    "        self.params = [p for p in agent.qnet.parameters()]\n",
    "        self.theta_star = [p.data for p in copy.deepcopy(self.params)]\n",
    "        matrix = [p.data*0 for p in copy.deepcopy(self.params)]\n",
    "        \n",
    "        for st in data:\n",
    "            agent.qnet.zero_grad()\n",
    "            output = agent.qnet(st.unsqueeze(0))\n",
    "            label = output.max(dim=1)[1]\n",
    "            lsm = torch.nn.functional.log_softmax(output, dim=1)\n",
    "            loss = torch.nn.functional.nll_loss(lsm, label)\n",
    "            temp = torch.autograd.grad(loss,self.params)\n",
    "    \n",
    "            for n in range(len(matrix)):\n",
    "                matrix[n]+=temp[n]**2\n",
    "\n",
    "        for n in range(len(matrix)):\n",
    "            matrix[n]/=data.shape[0]\n",
    "        self.fisher = matrix\n",
    "        print(\"Computing Fisher diagonal completed.\")\n",
    "\n",
    "    #Computing EWC loss using diagonal of fisher matrix  \n",
    "    #Based on https://arxiv.org/abs/1612.00796\n",
    "    def ewc_loss(self):\n",
    "        loss = 0\n",
    "        for n in range(len(self.fisher)):\n",
    "            loss+=(self.fisher[n]*(self.params[n]-self.theta_star[n])**2).sum()\n",
    "        return loss*self.lam\n",
    "    \n",
    "    def train(self):\n",
    "        self.qnet.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.qnet.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "    #Hard update target network\n",
    "    def target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data)\n",
    "     \n",
    "    #Soft update target network\n",
    "    def soft_target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data*self.tau + (1-self.tau)*target_net_params)\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.qnet(state), dim=1)  # choose greedy action\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        qsa_next_action = self.qnet_target(next_state)\n",
    "        qsa_next_action = torch.max(qsa_next_action, dim=1)[0]\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        \n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        #EWC loss\n",
    "        if self.ewc_on:\n",
    "            q_network_loss+=self.ewc_loss()\n",
    "\n",
    "        #Gradient descent\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "     \n",
    "    #call this to update Q network (train) and then make hard update of target network\n",
    "    def hard_update(self, update_rate):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(self.num_envs//3)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.update_count+=1\n",
    "            if self.update_count==self.target_update_interval:\n",
    "                self.target_update(self.qnet, self.qnet_target)\n",
    "                self.update_count=0\n",
    "                \n",
    "    #call this to update Q network (train) and then make soft update of target network\n",
    "    def update(self, update_rate, batch_size = 100):\n",
    "        if self.ewc_on:\n",
    "            if self.ewc_counter%(self.fisher_recompute_interval*update_rate)==0:\n",
    "                data = self.replay_buffer.sample_superbatch(3)[0]\n",
    "                self.compute_fisher_diagonal(data)\n",
    "                self.ewc_counter=0\n",
    "            self.ewc_counter+=1\n",
    "           \n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(batch_size)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.qnet, self.qnet_target)\n",
    "\n",
    "###############Simple DQN Agent######################################################            \n",
    "\n",
    "#################Double DQN Agent smooth##############################################\n",
    "#Based on https://arxiv.org/abs/1509.06461v3\n",
    "class DDQNAgent_smooth(DQNAgent):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800,\n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01):\n",
    "        super().__init__(NN, NN_args, num_envs, buffer_size, lr, discount, tau)\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        q_target_next_state_a = self.qnet_target(next_state)\n",
    "        q_target_next_state_max_a = torch.argmax(q_target_next_state_a, dim=1)\n",
    "        q_next_state_a = torch.gather(self.qnet(next_state), dim=1, index=q_target_next_state_max_a.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * q_next_state_a\n",
    "        \n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        if self.ewc_on:\n",
    "          q_network_loss+=self.ewc_loss()\n",
    "            \n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "            \n",
    "#################Double DQN Agent smooth##################################\n",
    "\n",
    "#################Double DQN Agent########################################\n",
    "#Based on https://arxiv.org/abs/1509.06461v1\n",
    "class DDQNAgent(DQNAgent):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800,\n",
    "                 lr: float = 0.0005, discount: float = 0.8):\n",
    "        #self.qnet = torch.load(\"dqn80x80.h5\")\n",
    "        self.Q_A = NN(*NN_args)\n",
    "        self.Q_B = NN(*NN_args)\n",
    "        self.Q_A_optim = torch.optim.Adam( self.Q_A.parameters(), lr=lr) #set learning rate\n",
    "        self.Q_B_optim = torch.optim.Adam( self.Q_B.parameters(), lr=lr) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = torch.nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size) #set size of replay buffer.\n",
    "        self.num_envs = num_envs\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        self.Q_A.train()\n",
    "        self.Q_A.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.Q_A.eval()\n",
    "        self.Q_B.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.Q_A(state), dim=1)  # choose greedy action\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_A_Network(self, state, next_state, action, reward, terminals):\n",
    "        QA_s_a = torch.gather(self.Q_A(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        QA_sn_a = self.Q_A(next_state)\n",
    "        QA_sn_a_max = torch.argmax(QA_sn_a, dim=1)\n",
    "        QB_sn_a = torch.gather(self.Q_B(next_state), dim=1, index=QA_sn_a_max.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        QA_s_a_target = reward + not_terminals * self.discount_factor * QB_sn_a\n",
    "        q_network_loss = self.MSELoss_function(QA_s_a, QA_s_a_target.detach())\n",
    "        self.Q_A_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.Q_A_optim.step()\n",
    "        \n",
    "    def update_Q_B_Network(self, state, next_state, action, reward, terminals):\n",
    "        QB_s_a = torch.gather(self.Q_B(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        QB_sn_a = self.Q_B(next_state)\n",
    "        QB_sn_a_max = torch.argmax(QB_sn_a, dim=1)\n",
    "        QA_sn_a = torch.gather(self.Q_A(next_state), dim=1, index=QB_sn_a_max.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        QB_s_a_target = reward + not_terminals * self.discount_factor * QA_sn_a\n",
    "        q_network_loss = self.MSELoss_function(QB_s_a, QB_s_a_target.detach())\n",
    "\n",
    "        self.Q_B_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.Q_B_optim.step()\n",
    "        \n",
    "    #call this to update Q network (train) and then make soft update of target network\n",
    "    def update(self, update_rate, batch_size):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(batch_size)\n",
    "            if np.random.uniform()<0.5:\n",
    "                self.update_Q_A_Network(states, next_states, actions, rewards, terminals)\n",
    "            else:\n",
    "                self.update_Q_B_Network(states, next_states, actions, rewards, terminals)\n",
    "#################Double DQN Agent##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fully connected neural network\n",
    "def FNN_1(shape, hidden_dim, action_dim):\n",
    "    flat_shape = np.product(shape) #length of the flattened state\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(flat_shape,hidden_dim),\n",
    "        #torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        #torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim, action_dim),\n",
    "         ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_1(): #A good model for SingleSnake\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_2():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_3():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(64, 128),\n",
    "        torch.nn.Dropout(0.4),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (4): Flatten()\n",
      "  (5): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (6): Dropout(p=0.4, inplace=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "environment = 'SingleSnake'\n",
    "num_envs = 1300 #Number of parallel environments to simulate. Use small value for cpu (eg. 1)\n",
    "test_num_envs = 100\n",
    "\n",
    "if environment == 'SimpleGridworld':\n",
    "    env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "    test_env = SimpleGridworld(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=DDQNAgent_smooth(NN = CNN_1, #NN_args = (state_dim, 512, action_dim),\n",
    "                           num_envs = num_envs, buffer_size = 600, lr = 0.0005,\n",
    "                           discount = 0.8, tau =0.01)\n",
    "\n",
    "\n",
    "elif environment == 'SingleSnake':\n",
    "    env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "    test_env = SingleSnake(num_envs=test_num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=DDQNAgent_smooth(NN = CNN_3, num_envs = num_envs, buffer_size = 600, lr = 0.0005, discount = 0.8, tau =0.01)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid option\")\n",
    "    \n",
    "agent.train()\n",
    "print(agent.qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000\n",
      "Episode Completed: 100 / 100\n",
      "Mean, Median, Max, Min, std: 5.75 5.0 22.0 0.0 4.0727754\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "render=False\n",
    "save_model = False\n",
    "number_of_steps = 100000\n",
    "epsilon = 1.0\n",
    "####Code to compute total reward####\n",
    "\n",
    "total_reward = torch.zeros(num_envs).to(DEFAULT_DEVICE)\n",
    "step_list=[]\n",
    "fc_list=[] #food collected\n",
    "best_fc = 0\n",
    "####Code to compute total reward####\n",
    "\n",
    "state=env.reset()\n",
    "agent.train()\n",
    "\n",
    "##########Filling the buffer###############################\n",
    "for i in range(30):\n",
    "    action = agent.epsilon_greedy_action( state , 1.0) \n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.add_to_buffer((state,next_state,action,reward,terminal))\n",
    "    state = next_state\n",
    "\n",
    "\n",
    "#Learning\n",
    "for i in range(1,number_of_steps):\n",
    "    ##############Learning######################\n",
    "    action = agent.epsilon_greedy_action( state , epsilon) \n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.add_to_buffer((state,next_state,action,reward,terminal))\n",
    "    agent.update(update_rate=10, batch_size=400)\n",
    "    state = next_state\n",
    "    \n",
    "    ##########EWC###############################\n",
    "    if i==2500: #When to turn on EWC\n",
    "        agent.ewc_on=True #Turning on EWC\n",
    "        agent.lam=400 #Setting scalar parameter in EWC loss function\n",
    "        agent.fisher_recompute_interval = 2000 #steps after which diagonal fisher matrix is recomputed\n",
    "    \n",
    "    ##########Changing Epsilon###################\n",
    "    if i==1000:\n",
    "        epsilon = 0.1\n",
    "    elif i==3000:\n",
    "        epsilon = 0.01\n",
    "    elif i==5000:\n",
    "        epsilon = 0.001\n",
    "    elif i==10000:\n",
    "        epsilon = 0.0001\n",
    "    elif i==15000:\n",
    "        epsilon = 0.00001\n",
    "    elif i==15000:\n",
    "        epsilon = 0.000001\n",
    "    \n",
    "    ###################Rendering###################\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    #############Validation############################\n",
    "    if (i%100==0):\n",
    "        #agent.evaluate()                        \n",
    "        t_state = test_env.reset()\n",
    "        fc_sum = torch.zeros((test_num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "        #hit_terminal = torch.zeros((test_num_envs,)).bool().to(DEFAULT_DEVICE)\n",
    "\n",
    "        for steps in range(1000): #max steps\n",
    "            t_action = agent.epsilon_greedy_action( t_state , 0.0)\n",
    "            t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "            #anything with a positive reward is considered as food.\n",
    "            fc_sum+=(t_reward>0).float()\n",
    "            #hit_terminal |= t_terminal\n",
    "            t_state = t_next_state\n",
    "            if t_terminal.all():\n",
    "                break\n",
    "\n",
    "        t_sum = fc_sum.cpu().numpy()\n",
    "        t_mean = np.mean(t_sum)\n",
    "        print('Step:', i)\n",
    "        print(\"Episode Completed:\", t_terminal.sum().cpu().numpy(), \"/\", test_num_envs)\n",
    "        print(\"Mean, Median, Max, Min, std:\", \n",
    "              t_mean, \n",
    "              np.median(t_sum),\n",
    "              np.max(t_sum),\n",
    "              np.min(t_sum),\n",
    "              np.std(t_sum))\n",
    "        fc_list.append(t_mean)\n",
    "        step_list.append(i)\n",
    "        plt.plot(step_list, fc_list)\n",
    "        plt.show()\n",
    "        agent.train()\n",
    "        clear_output(wait=True)\n",
    "        if t_mean>best_fc:\n",
    "            best_fc = t_mean\n",
    "            torch.save(agent.qnet,\"models/best_model.h5\")\n",
    "        \n",
    "#########Closing renderer if open########################    \n",
    "if render:    \n",
    "    env.close()\n",
    "\n",
    "##########Saving model##########################   \n",
    "if save_model:\n",
    "    model = torch.load(\"best_model.h5\")\n",
    "    with open('models/best_model.pickle', 'wb') as f:\n",
    "        pickle.dump((model, step_list,fc_list), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data of best model and associated runtime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing reward\n",
    "plt.plot(step_list, fc_list)\n",
    "plt.show()\n",
    "\n",
    "#agent.qnet = torch.load(\"models/bestest_model(1).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualizing reward\n",
    "plt.plot(step_list, fc_list)\n",
    "plt.show()\n",
    "\n",
    "#Store data about the best model as a pickle file\n",
    "import pickle\n",
    "model = torch.load(\"models/best_model.h5\")\n",
    "with open('models/cnn_256_avg_822.pickle', 'xb') as f:\n",
    "    pickle.dump((model, step_list,reward_list), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the saved model\n",
    "with open('models/bestest_model.pickle', 'rb') as f:\n",
    "     data = pickle.load(f)\n",
    "\n",
    "model = data[0].to(DEFAULT_DEVICE)        \n",
    "agent.qnet= data[0]\n",
    "\n",
    "plt.plot(data[1], data[2])\n",
    "plt.show()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.qnet = torch.load(\"models/best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Record Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "agent.evaluate()\n",
    "PATH = os.getcwd()\n",
    "state = env.reset()\n",
    "for episode in range(100):\n",
    "    fc_sum = 0\n",
    "    recorder = VideoRecorder(env, path=PATH + f'/videos/snake_{episode}.mp4')\n",
    "    #env.render()\n",
    "    recorder.capture_frame()\n",
    "    time.sleep(0.2)\n",
    "    counter = 0\n",
    "    while(1):\n",
    "        counter+=1\n",
    "        action = agent.epsilon_greedy_action( state , 0.0)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        fc_sum+= (reward>0).cpu().numpy()\n",
    "        #env.render()\n",
    "        recorder.capture_frame()\n",
    "        #time.sleep(0.2)\n",
    "        state = next_state\n",
    "        if terminal.all() or counter==1000:\n",
    "            recorder.close()\n",
    "            break\n",
    "    print(\"Completed:\", terminal.any().cpu().numpy())\n",
    "    print('Episode:', episode, 'Food Collected:', fc_sum)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Average Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "agent.evaluate()\n",
    "\n",
    "                       \n",
    "t_state = test_env.reset()\n",
    "fc_sum = torch.zeros((num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "\n",
    "for steps in range(1000): #max steps\n",
    "    t_action = agent.epsilon_greedy_action( t_state , 0.0)\n",
    "    t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "    #anything with a positive reward is considered as food.\n",
    "    fc_sum+=(t_reward>0).float()\n",
    "    t_state = t_next_state\n",
    "    if t_terminal.all():\n",
    "        break\n",
    "\n",
    "t_sum = fc_sum.cpu().numpy()\n",
    "t_mean = np.mean(t_sum)\n",
    "print(\"Completed:\", t_terminal.sum().cpu().numpy())\n",
    "print(\"Mean, Median, Max, Min, std:\", \n",
    "      t_mean, \n",
    "      np.median(t_sum),\n",
    "      np.max(t_sum),\n",
    "      np.min(t_sum),\n",
    "      np.std(t_sum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
