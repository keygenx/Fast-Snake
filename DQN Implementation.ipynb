{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os #to get current working directory\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wurm.envs import SingleSnake\n",
    "from wurm.envs import SimpleGridworld\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "DEFAULT_DEVICE = 'cuda' #set device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size: int):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer = collections.deque(maxlen=max_buffer_size)\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "    def clear_buffer(self):\n",
    "        self.buffer.clear()\n",
    "        \n",
    "    #Sample superbatches and sub sample parallel environments\n",
    "    def sample_subbatch(self,superbatch_length, subbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        sub_length = self.buffer[0][0].shape[0]\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), superbatch_length):\n",
    "            rand_int_1 = np.random.randint(0, sub_length, subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0][rand_int_1])\n",
    "            next_states.append(transition[1][rand_int_1])\n",
    "            actions.append(transition[2][rand_int_1])\n",
    "            rewards.append(transition[3][rand_int_1])\n",
    "            terminals.append(transition[4][rand_int_1])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "\n",
    "    def sample_superbatch(self,superbatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for rand_int in np.random.randint(0, len(self.buffer), superbatch_length):\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)\n",
    "    \n",
    "    #sample parallel environments of subbatch_length from a randomly selected buffer location.\n",
    "    def sample(self, subbatch_length):\n",
    "            rand_int = np.random.randint(0, len(self.buffer))\n",
    "            rand_int_1 = np.random.randint(0, len(self.buffer[0][0]), subbatch_length)\n",
    "            transition = self.buffer[rand_int]\n",
    "            states=transition[0][rand_int_1]\n",
    "            next_states=transition[1][rand_int_1]\n",
    "            actions=transition[2][rand_int_1]\n",
    "            rewards=transition[3][rand_int_1]\n",
    "            terminals=transition[4][rand_int_1]\n",
    "            return (states,next_states,actions,rewards,terminals)\n",
    "\n",
    "        \n",
    "#A buffer with lesser correlation between samples. Implemented with pytorch. \n",
    "#Presently not working properly. Not sure why.\n",
    "class BetterBuffer():\n",
    "    def __init__(self, max_envs: int = 1000):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer_0 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_1 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_2 = torch.empty(0).long().to(DEFAULT_DEVICE)\n",
    "        self.buffer_3 = torch.empty(0).to(DEFAULT_DEVICE)\n",
    "        self.buffer_4 = torch.empty(0).bool().to(DEFAULT_DEVICE)\n",
    "        self.max_length = max_envs\n",
    "        self.pointer = 0\n",
    "        self.full = False\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        if self.full == True:\n",
    "            if self.pointer==self.max_length:\n",
    "                self.pointer=0\n",
    "            self.buffer_0[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[0]\n",
    "            self.buffer_1[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[1]\n",
    "            self.buffer_2[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[2]\n",
    "            self.buffer_3[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[3]\n",
    "            self.buffer_4[self.pointer*num_envs:(self.pointer+1)*num_envs] = data[4]\n",
    "            self.pointer+=1\n",
    "        else:\n",
    "            self.buffer_0=torch.cat((self.buffer_0,data[0]))\n",
    "            self.buffer_1=torch.cat((self.buffer_1,data[1]))\n",
    "            self.buffer_2=torch.cat((self.buffer_2,data[2]))\n",
    "            self.buffer_3=torch.cat((self.buffer_3,data[3]))\n",
    "            self.buffer_4=torch.cat((self.buffer_4,data[4]))\n",
    "            self.pointer+=1\n",
    "            if self.pointer==self.max_length:\n",
    "                self.full=True\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if self.full == True:\n",
    "            randint = torch.randint(0, self.max_length*num_envs,(batch_size,))\n",
    "        else:\n",
    "            randint = torch.randint(0,self.pointer*num_envs, (batch_size,))\n",
    "        return self.buffer_0[randint], self.buffer_1[randint], self.buffer_2[randint], self.buffer_3[randint], self.buffer_4[randint]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################Simple DQN Agent########################################\n",
    "class DQNAgent():\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800, \n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01,\n",
    "                 lam = 10):\n",
    "        #self.qnet = torch.load(\"dqn80x80.h5\")\n",
    "        self.qnet = NN(*NN_args)\n",
    "        \n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "        for param in self.qnet_target.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        self.qnet_optim = torch.optim.Adam( self.qnet.parameters(), lr=lr) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = torch.nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size) #set size of replay buffer.\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.target_update_interval = 500 #set for target update interval for hard target network updates\n",
    "        self.update_count= 0 #internal working variable. Don't change\n",
    "        self.tau = tau # set tau for soft target network updates\n",
    "        self.num_envs = num_envs\n",
    "        \n",
    "        self.ewc_counter = 0 #flag keep track of fisher matrix computations.\n",
    "        self.lam = torch.Tensor([lam]).to(DEFAULT_DEVICE)\n",
    "        self.ewc_on = False\n",
    "        self.fisher_recompute_interval = 3000\n",
    "        \n",
    "    def add_to_buffer(self, data):\n",
    "        self.replay_buffer.add_to_buffer(data)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.qnet = torch.load(path, map_location = DEFAULT_DEVICE)\n",
    "        self.qnet_target = torch.load(path, map_location = DEFAULT_DEVICE)\n",
    "        self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=self.lr)\n",
    "        \n",
    "    #computing diagonal of fisher matrix\n",
    "    def compute_fisher_diagonal(self, data):\n",
    "        print(\"Computing Fisher diagonal...\")\n",
    "        self.params = [p for p in agent.qnet.parameters()]\n",
    "        self.theta_star = [p.data for p in copy.deepcopy(self.params)]\n",
    "        matrix = [p.data*0 for p in copy.deepcopy(self.params)]\n",
    "        \n",
    "        for st in data:\n",
    "            agent.qnet.zero_grad()\n",
    "            output = agent.qnet(st.unsqueeze(0))\n",
    "            label = output.max(dim=1)[1]\n",
    "            lsm = torch.nn.functional.log_softmax(output, dim=1)\n",
    "            loss = torch.nn.functional.nll_loss(lsm, label)\n",
    "            temp = torch.autograd.grad(loss,self.params)\n",
    "    \n",
    "            for n in range(len(matrix)):\n",
    "                matrix[n]+=temp[n]**2\n",
    "\n",
    "        for n in range(len(matrix)):\n",
    "            matrix[n]/=data.shape[0]\n",
    "        self.fisher = matrix\n",
    "        print(\"Computing Fisher diagonal completed.\")\n",
    "\n",
    "    #Computing EWC loss using diagonal of fisher matrix  \n",
    "    #Based on https://arxiv.org/abs/1612.00796\n",
    "    def ewc_loss(self):\n",
    "        loss = 0\n",
    "        for n in range(len(self.fisher)):\n",
    "            loss+=(self.fisher[n]*(self.params[n]-self.theta_star[n])**2).sum()\n",
    "        return loss*self.lam\n",
    "    \n",
    "    def train(self):\n",
    "        self.qnet.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.qnet.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "    #Hard update target network\n",
    "    def target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data)\n",
    "     \n",
    "    #Soft update target network\n",
    "    def soft_target_update(self,network,target_network):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data*self.tau + (1-self.tau)*target_net_params)\n",
    "    \n",
    "    def epsilon_greedy_action(self, env, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.qnet(state), dim=1)  # choose greedy action\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        qsa_next_action = self.qnet_target(next_state)\n",
    "        qsa_next_action = torch.max(qsa_next_action, dim=1)[0]\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        \n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        #EWC loss\n",
    "        if self.ewc_on:\n",
    "            q_network_loss+=self.ewc_loss()\n",
    "\n",
    "        #Gradient descent\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "     \n",
    "    #call this to update Q network (train) and then make hard update of target network\n",
    "    def hard_update(self, update_rate):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(self.num_envs//3)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.update_count+=1\n",
    "            if self.update_count==self.target_update_interval:\n",
    "                self.target_update(self.qnet, self.qnet_target)\n",
    "                self.update_count=0\n",
    "                \n",
    "    #call this to update Q network (train) and then make soft update of target network\n",
    "    def update(self, update_rate, batch_size = 100):\n",
    "        if self.ewc_on:\n",
    "            if self.ewc_counter%(self.fisher_recompute_interval*update_rate)==0:\n",
    "                data = self.replay_buffer.sample_superbatch(3)[0]\n",
    "                self.compute_fisher_diagonal(data)\n",
    "                self.ewc_counter=0\n",
    "            self.ewc_counter+=1\n",
    "           \n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(batch_size)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.qnet, self.qnet_target)\n",
    "\n",
    "###############Simple DQN Agent######################################################            \n",
    "\n",
    "#################Double DQN Agent smooth##############################################\n",
    "#Based on https://arxiv.org/abs/1509.06461v3\n",
    "class DDQNAgent_smooth(DQNAgent):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800,\n",
    "                 lr: float = 0.0005, discount: float = 0.8, tau: float = 0.01):\n",
    "        super().__init__(NN, NN_args, num_envs, buffer_size, lr, discount, tau)\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        q_target_next_state_a = self.qnet_target(next_state)\n",
    "        q_target_next_state_max_a = torch.argmax(q_target_next_state_a, dim=1)\n",
    "        q_next_state_a = torch.gather(self.qnet(next_state), dim=1, index=q_target_next_state_max_a.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * q_next_state_a\n",
    "        \n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        if self.ewc_on:\n",
    "            q_network_loss+=self.ewc_loss()\n",
    "            \n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "            \n",
    "#################Double DQN Agent smooth##################################\n",
    "\n",
    "#################Double DQN Agent########################################\n",
    "#Code discontinued. Will not be updated.\n",
    "#Based on https://arxiv.org/abs/1509.06461v1\n",
    "class DDQNAgent(DQNAgent):\n",
    "    def __init__(self, NN: object, NN_args: tuple = (), \n",
    "                 num_envs: int = 1, buffer_size: int = 800,\n",
    "                 lr: float = 0.0005, discount: float = 0.8):\n",
    "        #self.qnet = torch.load(\"dqn80x80.h5\")\n",
    "        self.Q_A = NN(*NN_args)\n",
    "        self.Q_B = NN(*NN_args)\n",
    "        self.Q_A_optim = torch.optim.Adam( self.Q_A.parameters(), lr=lr) #set learning rate\n",
    "        self.Q_B_optim = torch.optim.Adam( self.Q_B.parameters(), lr=lr) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([discount]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = torch.nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size) #set size of replay buffer.\n",
    "        self.num_envs = num_envs\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        self.Q_A.train()\n",
    "        self.Q_A.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.Q_A.eval()\n",
    "        self.Q_B.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.Q_A(state), dim=1)  # choose greedy action\n",
    "    \n",
    "    #update Q Network by calculating gradient of neural network loss\n",
    "    def update_Q_A_Network(self, state, next_state, action, reward, terminals):\n",
    "        QA_s_a = torch.gather(self.Q_A(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        QA_sn_a = self.Q_A(next_state)\n",
    "        QA_sn_a_max = torch.argmax(QA_sn_a, dim=1)\n",
    "        QB_sn_a = torch.gather(self.Q_B(next_state), dim=1, index=QA_sn_a_max.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        QA_s_a_target = reward + not_terminals * self.discount_factor * QB_sn_a\n",
    "        q_network_loss = self.MSELoss_function(QA_s_a, QA_s_a_target.detach())\n",
    "        self.Q_A_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.Q_A_optim.step()\n",
    "        \n",
    "    def update_Q_B_Network(self, state, next_state, action, reward, terminals):\n",
    "        QB_s_a = torch.gather(self.Q_B(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        QB_sn_a = self.Q_B(next_state)\n",
    "        QB_sn_a_max = torch.argmax(QB_sn_a, dim=1)\n",
    "        QA_sn_a = torch.gather(self.Q_A(next_state), dim=1, index=QB_sn_a_max.unsqueeze(-1)).squeeze()\n",
    "        not_terminals = ~terminals\n",
    "        QB_s_a_target = reward + not_terminals * self.discount_factor * QA_sn_a\n",
    "        q_network_loss = self.MSELoss_function(QB_s_a, QB_s_a_target.detach())\n",
    "\n",
    "        self.Q_B_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.Q_B_optim.step()\n",
    "        \n",
    "    #call this to update Q network (train) and then make soft update of target network\n",
    "    def update(self, update_rate, batch_size):\n",
    "        for _ in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample(batch_size)\n",
    "            if np.random.uniform()<0.5:\n",
    "                self.update_Q_A_Network(states, next_states, actions, rewards, terminals)\n",
    "            else:\n",
    "                self.update_Q_B_Network(states, next_states, actions, rewards, terminals)\n",
    "#################Double DQN Agent##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fully connected neural network\n",
    "def FNN_1(shape, hidden_dim, action_dim):\n",
    "    flat_shape = np.product(shape) #length of the flattened state\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(flat_shape,hidden_dim),\n",
    "        #torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        #torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim, action_dim),\n",
    "         ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_1(): #A good model for SingleSnake\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_2():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "\n",
    "def CNN_3(): #A good model for SingleSnake\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1)),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(200, 128),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 4),\n",
    "        ).to(DEFAULT_DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): Dropout(p=0.1, inplace=False)\n",
      "  (4): ReLU()\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): Dropout(p=0.2, inplace=False)\n",
      "  (7): ReLU()\n",
      "  (8): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (9): Flatten()\n",
      "  (10): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (11): Dropout(p=0.5, inplace=False)\n",
      "  (12): ReLU()\n",
      "  (13): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "environment = 'SingleSnake'\n",
    "num_envs = 1300 #Number of parallel environments to simulate. Use small value for cpu (eg. 1)\n",
    "test_num_envs = 100\n",
    "discount = 0.99\n",
    "tau = 0.1\n",
    "lr = 0.001\n",
    "env_size = 10\n",
    "buffer_size = 600\n",
    "\n",
    "if environment == 'SimpleGridworld':\n",
    "    env = SimpleGridworld(num_envs=num_envs, size=env_size, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "    test_env = SimpleGridworld(num_envs=test_num_envs, size=env_size, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=DDQNAgent_smooth(NN = FNN_1, NN_args = (state_dim, 512, action_dim),\n",
    "                           num_envs = num_envs, buffer_size = buffer_size, lr = lr,\n",
    "                           discount = discount, tau = tau)\n",
    "\n",
    "\n",
    "elif environment == 'SingleSnake':\n",
    "    env = SingleSnake(num_envs=num_envs, size=env_size, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "    test_env = SingleSnake(num_envs=test_num_envs, size=env_size, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "\n",
    "    state = env.reset()\n",
    "    state_dim = state.shape[1:]\n",
    "    action_dim = 4\n",
    "\n",
    "    #Effective buffer_size = buffer_size*num_envs\n",
    "    agent=DDQNAgent_smooth(NN = CNN_1, num_envs = num_envs, buffer_size = buffer_size, lr = lr, discount = discount, tau = tau)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid option\")\n",
    "\n",
    "#agent.load(\"models/best_model.h5\")\n",
    "agent.train()\n",
    "print(agent.qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b51c9386e30b>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, update_rate, batch_size)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_Q_Network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_target_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnet_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-74ecefdf0b0b>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, subbatch_length)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mrand_int_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubbatch_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mtransition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_int\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mstates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_int_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mnext_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_int_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_int_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "number_of_steps = 100000\n",
    "epsilon = 1.0\n",
    "update_rate = 10\n",
    "batch_size = 400\n",
    "####Code to compute total reward####\n",
    "\n",
    "total_reward = torch.zeros(num_envs).to(DEFAULT_DEVICE)\n",
    "step_list=[]\n",
    "fc_list=[] #food collected\n",
    "data_list = []\n",
    "best_fc = 0\n",
    "####Code to compute total reward####\n",
    "\n",
    "state=env.reset()\n",
    "agent.train()\n",
    "#agent.qnet.eval()\n",
    "##########Filling the buffer###############################\n",
    "for i in range(30):\n",
    "    action = agent.epsilon_greedy_action(env, state , 1.0) \n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.add_to_buffer((state,next_state,action,reward,terminal))\n",
    "    state = next_state\n",
    "\n",
    "\n",
    "#Learning\n",
    "for i in range(1,number_of_steps):\n",
    "    ##############Learning######################\n",
    "    action = agent.epsilon_greedy_action(env, state , epsilon) \n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    agent.add_to_buffer((state,next_state,action,reward,terminal))\n",
    "    agent.update(update_rate=update_rate, batch_size=batch_size)\n",
    "    state = next_state\n",
    "    \n",
    "    ##########EWC###############################\n",
    "    if i==-1: #When to turn on EWC\n",
    "        agent.ewc_on=True #Turning on EWC\n",
    "        agent.lam=400 #Setting scalar parameter in EWC loss function\n",
    "        agent.fisher_recompute_interval = 2000 #steps after which diagonal fisher matrix is recomputed\n",
    "    \n",
    "    ##########Changing Epsilon###################\n",
    "    if i==1000:\n",
    "        epsilon = 0.1\n",
    "    elif i==3000:\n",
    "        epsilon = 0.01\n",
    "    elif i==5000:\n",
    "        epsilon = 0.001\n",
    "    elif i==30000:\n",
    "        epsilon = 0.0001\n",
    "    elif i==150000:\n",
    "        epsilon = 0.00001\n",
    "    \n",
    "    #############Validation and data collection############################\n",
    "    if (i%100==0):\n",
    "        agent.evaluate()                        \n",
    "        t_state = test_env.reset()\n",
    "        fc_sum = torch.zeros((test_num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "        #hit_terminal = torch.zeros((test_num_envs,)).bool().to(DEFAULT_DEVICE)\n",
    "\n",
    "        for steps in range(1000): #max steps\n",
    "            t_action = agent.epsilon_greedy_action(test_env, t_state , 0.0)\n",
    "            t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "            #anything with a positive reward is considered as food.\n",
    "            fc_sum+=(t_reward>0).float()\n",
    "            t_state = t_next_state\n",
    "            if t_terminal.all():\n",
    "                break\n",
    "\n",
    "        t_sum = fc_sum.cpu().numpy()\n",
    "        t_mean = np.mean(t_sum)\n",
    "        t_median = np.median(t_sum)\n",
    "        t_max = np.max(t_sum)\n",
    "        t_min = np.min(t_sum)\n",
    "        t_std = np.std(t_sum)\n",
    "        t_completed = t_terminal.sum().cpu().numpy()\n",
    "        print('Step:', i)\n",
    "        print(\"Episode Completed:\", t_completed, \"/\", test_num_envs)\n",
    "        print(\"Mean, Median, Max, Min, std:\", \n",
    "              t_mean, \n",
    "              t_median,\n",
    "              t_max,\n",
    "              t_min,\n",
    "              t_std)\n",
    "        fc_list.append(t_mean)\n",
    "        data_list.append((i, t_mean, t_median, t_min, t_max, t_std, t_sum))\n",
    "        step_list.append(i)\n",
    "        plt.plot(step_list, fc_list)\n",
    "        plt.show()\n",
    "        agent.train()\n",
    "        clear_output(wait=True)\n",
    "        if t_mean>best_fc:\n",
    "            best_fc = t_mean\n",
    "            torch.save(agent.qnet,\"current_best.h5\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data of best model and associated runtime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing reward\n",
    "plt.plot(step_list, fc_list)\n",
    "plt.show()\n",
    "\n",
    "#file name without extension\n",
    "file_name = \"test\"\n",
    "\n",
    "parameters = {\"environment\": environment,\n",
    "              \"env_size\": env_size,\n",
    "              \"num_envs\": num_envs, \n",
    "              \"discount\": discount, \n",
    "              \"tau\": tau, \n",
    "              \"lr\": lr,\n",
    "              \"update_rate\": update_rate,\n",
    "              \"batch_size\": batch_size,\n",
    "              \"device\": DEFAULT_DEVICE\n",
    "             }\n",
    "heading = [\"step\", \"mean\", \"median\", \"min\", \"max\", \"std\", \"raw data\"]\n",
    "\n",
    "#create folder if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "with open(F\"data/{file_name}.csv\", 'xt') as file:\n",
    "    writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "    writer.writerow([\"parameters:\"])\n",
    "    writer.writerow(parameters.keys())\n",
    "    writer.writerow(parameters.values())\n",
    "    writer.writerow([\"DATA:\"])\n",
    "    writer.writerow(heading)\n",
    "    for item in data_list:\n",
    "        writer.writerow(item[0:-1]+tuple(item[-1]))\n",
    "    \n",
    "    \n",
    "#file name without extension\n",
    "\n",
    "#Store data about the best model as a pickle file\n",
    "model = torch.load(\"current_best.h5\")\n",
    "model = model.to('cpu')\n",
    "torch.save(model, F\"data/{file_name}.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "{'environment': 'SingleSnake', 'env_size': '10', 'num_envs': '1300', 'discount': '0.99', 'tau': '0.1', 'lr': '0.001', 'update_rate': '10', 'batch_size': '400', 'device': 'cuda'}\n",
      "\n",
      "Model:\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): Dropout(p=0.1, inplace=False)\n",
      "  (4): ReLU()\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): Dropout(p=0.2, inplace=False)\n",
      "  (7): ReLU()\n",
      "  (8): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (9): Flatten()\n",
      "  (10): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (11): Dropout(p=0.5, inplace=False)\n",
      "  (12): ReLU()\n",
      "  (13): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Mean collected food:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#file name to read\n",
    "file_name = \"test\"\n",
    "\n",
    "parameters = {}\n",
    "heading = [\"step\", \"mean\", \"median\", \"min\", \"max\", \"std\", \"raw data\"]\n",
    "\n",
    "with open(F\"data/{file_name}.csv\", 'rt') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = [row for row in reader]\n",
    "    for key,value in zip(data[1],data[2]):\n",
    "        parameters[key] = value\n",
    "    data_list = []\n",
    "    step_list=[]\n",
    "    fc_list=[]\n",
    "    for items in data[5:]:\n",
    "        nitems = [float(i) for i in items]\n",
    "        step_list.append(nitems[0])\n",
    "        fc_list.append(nitems[1])\n",
    "        data_list.append(tuple(nitems[0:6])+(np.array(nitems[6:]),))\n",
    "        \n",
    "\n",
    "agent.load(F\"data/{file_name}.h5\")\n",
    "print(\"Parameters:\")\n",
    "print(parameters)\n",
    "print(\"\\nModel:\")\n",
    "print(agent.qnet)\n",
    "\n",
    "print(\"\\nMean collected food:\")\n",
    "#Plotting\n",
    "plt.plot(step_list, fc_list, label = \"Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Record Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "agent.evaluate()\n",
    "PATH = os.getcwd()\n",
    "state = env.reset()\n",
    "for episode in range(100):\n",
    "    fc_sum = 0\n",
    "    recorder = VideoRecorder(env, path=PATH + f'/videos/snake_{episode}.mp4')\n",
    "    #env.render()\n",
    "    recorder.capture_frame()\n",
    "    time.sleep(0.2)\n",
    "    counter = 0\n",
    "    while(1):\n",
    "        counter+=1\n",
    "        action = agent.epsilon_greedy_action(env, state , 0.0)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        fc_sum+= (reward>0).cpu().numpy()\n",
    "        #env.render()\n",
    "        recorder.capture_frame()\n",
    "        #time.sleep(0.2)\n",
    "        state = next_state\n",
    "        if terminal.all() or counter==1000:\n",
    "            recorder.close()\n",
    "            break\n",
    "    print(\"Completed:\", terminal.any().cpu().numpy())\n",
    "    print('Episode:', episode, 'Food Collected:', fc_sum)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Average Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = SimpleGridworld(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE, auto_reset=False)\n",
    "agent.evaluate()\n",
    "\n",
    "                       \n",
    "t_state = test_env.reset()\n",
    "fc_sum = torch.zeros((num_envs,)).float().to(DEFAULT_DEVICE) #foot collected\n",
    "\n",
    "for steps in range(1000): #max steps\n",
    "    t_action = agent.epsilon_greedy_action(test_env, t_state , 0.0)\n",
    "    t_next_state, t_reward, t_terminal, _ = test_env.step(t_action)\n",
    "    #anything with a positive reward is considered as food.\n",
    "    fc_sum+=(t_reward>0).float()\n",
    "    t_state = t_next_state\n",
    "    if t_terminal.all():\n",
    "        break\n",
    "\n",
    "t_sum = fc_sum.cpu().numpy()\n",
    "t_mean = np.mean(t_sum)\n",
    "print(\"Completed:\", t_terminal.sum().cpu().numpy())\n",
    "print(\"Mean, Median, Max, Min, std:\", \n",
    "      t_mean, \n",
    "      np.median(t_sum),\n",
    "      np.max(t_sum),\n",
    "      np.min(t_sum),\n",
    "      np.std(t_sum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
