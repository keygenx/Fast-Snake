{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from wurm.envs import SingleSnake\n",
    "DEFAULT_DEVICE = 'cpu' #set device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fully connected neural network\n",
    "def Q_Network(shape, action_dim, hidden_dim):\n",
    "    flat_shape = np.product(shape) #length of the flattened state\n",
    "    model = nn.Sequential(\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(flat_shape,hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim,hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, action_dim),\n",
    "                         ).to(DEFAULT_DEVICE)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "state = env.reset()\n",
    "state_dim = state.shape[1:]\n",
    "action_dim = 4\n",
    "qnet=Q_Network(state_dim, 4, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network. Requires Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30600), started 14:01:49 ago. (Use '!kill 30600' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-91ec07c9c5af095b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-91ec07c9c5af095b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(qnet, torch.Tensor(env.reset()))\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size: int):\n",
    "        self.buffer = collections.deque()\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        if(len(self.buffer)==self.max_buffer_size):\n",
    "            self.buffer.pop()\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "        \n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for rand_int in np.random.randint(0, len(self.buffer)-1, minibatch_length):\n",
    "            transition = self.buffer[rand_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.cat(states), torch.cat(next_states), torch.cat(actions), torch.cat(rewards), torch.cat(terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.qnet = Q_Network(state_dim, action_dim, 50) #Set diminesion of hidden layer\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "        self.qnet_optim = torch.optim.Adam( self.qnet.parameters(), lr=0.0005) #set learning rate\n",
    "        self.discount_factor = torch.Tensor([0.99]).to(DEFAULT_DEVICE) # set discount factor\n",
    "        self.MSELoss_function = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(1000) #set size of replay buffer.\n",
    "        self.tau = torch.Tensor([0.95]).to(DEFAULT_DEVICE) #set soft update tau.\n",
    "        pass\n",
    "    \n",
    "    def soft_target_update(self,network,target_network,tau):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data * tau + target_net_params.data * (1 - tau))\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "                return env.random_action()  # choose random action\n",
    "        else:                \n",
    "            return torch.argmax(self.qnet(state), dim=1)  # choose greedy action\n",
    "\n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.unsqueeze(-1)).squeeze()\n",
    "        qsa_next_action = self.qnet_target(next_state)\n",
    "        qsa_next_action = torch.max(qsa_next_action, dim=1)[0]\n",
    "        not_terminals = ~terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_buffer.sample_minibatch(128)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.qnet, self.qnet_target, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten()\n",
       "  (1): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=50, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_envs = 1\n",
    "env = SingleSnake(num_envs=num_envs, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "state = env.reset()\n",
    "state_dim = state.shape[1:]\n",
    "action_dim = 4\n",
    "agent=DQNAgent(state_dim,action_dim)\n",
    "agent.qnet.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "render=False\n",
    "\n",
    "number_of_episodes = 1000\n",
    "\n",
    "reward_sum=torch.zeros([num_envs]).to(DEFAULT_DEVICE)\n",
    "state=env.reset()\n",
    "agent.qnet.train()\n",
    "\n",
    "for i in range(1,number_of_episodes):\n",
    "    action = agent.epsilon_greedy_action( state , 0.5-0.5*i/number_of_episodes) #set epsilon\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    reward_sum+=reward\n",
    "    agent.replay_buffer.add_to_buffer( (state,next_state,action,reward,terminal) )\n",
    "    state = next_state\n",
    "    if render:\n",
    "        env.render()\n",
    "    if terminal.any():\n",
    "        print('episode:', i, 'sum_of_rewards_for_episode:', reward_sum[terminal])        \n",
    "        clear_output(wait=True)\n",
    "        state=env.reset(terminal)\n",
    "        reward_sum[terminal]=0\n",
    "    if i>3:\n",
    "        agent.update(2)\n",
    "\n",
    "if render:    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 sum_of_rewards_for_episode: [0.]\n",
      "episode: 1 sum_of_rewards_for_episode: [0.]\n",
      "episode: 2 sum_of_rewards_for_episode: [0.]\n",
      "episode: 3 sum_of_rewards_for_episode: [1.]\n",
      "episode: 4 sum_of_rewards_for_episode: [0.]\n"
     ]
    }
   ],
   "source": [
    "env = SingleSnake(num_envs=1, size=10, observation_mode='one_channel', device= DEFAULT_DEVICE)\n",
    "agent.qnet.eval()\n",
    "for episode in range(5):\n",
    "    reward_sum = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while(1):\n",
    "        action = agent.epsilon_greedy_action( state , 0.0)\n",
    "        #print(agent.qnet(state))\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        reward_sum+= reward.numpy()\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        state = next_state\n",
    "        if terminal:\n",
    "            break\n",
    "    print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
